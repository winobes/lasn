\documentclass[12pt]{scrartcl}

\usepackage[]{natbib}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage[paperwidth=210mm,
            paperheight=297mm,
            left=50pt,
            top=50pt,
            textwidth=345pt,
            marginparsep=25pt,
            marginparwidth=124pt,
            textheight=692pt,
            footskip=50pt]
           {geometry}
\usepackage[textsize=footnotesize]{todonotes}
%\usepackage{showframe}

\graphicspath{ {./figures/} }
\setlist{noitemsep}

\newcommand{\bill}[1]{\todo[color=blue!40]{#1}}
\newcommand{\raquel}[1]{\todo[color=orange!40]{#1}}

\title{Linguistic alignment in social networks}
\subtitle{Work in progress // Notes}

\date{\today}

\begin{document}
\maketitle

\section{References // Background}

\subsection{Linguistic style alignment}
\begin{itemize}
  \item Examples of alignment -- phonetic production \citep{kim_phonetic_2011,babel_evidence_2012}, lexical choice \citep{brennan_conceptual_1996}, use of function words \citep{niederhoffer_linguistic_2002}, syntactic constructions \citep{pickering_structural_2008}. 
  \item Alignment is motivated by mutual understanding / common ground -- \cite{clark_using_1996,clark_audience_1982,brennan_conceptual_1996,brennan_partner-specific_2009}
  \item Alignment is explained by stimulus response priming --  \cite{pickering_interactive-alignment_2004,branigan_syntactic_1995,pickering_alignment_2006,reitter_alignment_2014} 
  \item{\cite{muir_characterizing_2016} -- psychology experiment on linguistic style accommodation with 144 face-to-face dyads; manipulated social power \& personality traits}
\end{itemize}

\subsection{Marker-based measures of linguistic style alignment}
Markers: \cite{tausczik_psychological_2010,niederhoffer_linguistic_2002}

\subsubsection{Linguistic style matching}

\cite{ireland_language_2011} uses the ratio of two speakers' marker frequency to measure their style alignment.

\[
LSM^m = 1 - \frac{{count(a,m) - count(b,m)}}{count(a,m) + count(b,m) + .00001}
\]
This is not really a measure of coordination since there is no analysis of whether $a$'s use of $m$ \emph{triggers} $b$'s use.
Instead, it just measures how well aligned $a$ and $b$'s rates of $m$-use are.
There is no accounting for the speakers' baseline marker use.
They also do not attempt to quantify convergence, instead taking the marker use rates over an entire conversation.

\subsubsection{Subtractive conditional probability}

\begin{itemize}
  \item first defined by \cite{danescu-niculescu-mizil_mark_2011}
  \item adapted to a group setting by \cite{danescu-niculescu-mizil_echoes_2012}.
\end{itemize}

The goal is to determine for a given user pair of speakers $a$ and $b$ and stylistic marker $m$, if $a$'s use of $m$ increases the probability that $b$ will use $m$ (beyond the baseline probability of $m$ for $b$).
Given a set of exchanges where $u_a$ is an utterance by speaker $a$ and $u_b$ is an utterance by $b$, the coordination of $b$ towards $a$ along marker $m$ is defined as:
\[
  C^m(b,a) = P(\mathcal{E}^{m}_{u_b} \mid \mathcal{E}^m_{u_a}) - P(\mathcal{E}^m_{u_b})
\]
where the conditional probability and the baseline probability are both estimated from the set of exchanges where $b$ replies to $a$.

$C^m(b,a)$ is undefined if no $u_a$ exhibits $m$.

Coordination towards a group $A$ is defined by simply modifying the set of exchanges used to estimate probabilities to include any pair of utterances where $b$ replies to any user $a\in A$.

$C(B,A)$ is computed by first taking the macro average across markers, $C(b,A) = \sum_m C^m(b,A)$, and then taking the average across $b\in B$.
The authors give three ways of dealing with missing data when aggregating across markers:
\begin{itemize}
  \item Agg1 -- only consider $C(b,A)$ where $C^m(b,A)$ is defined for every $m$.
  \item Agg2 -- if $C^m(b,A)$ is undefined, smooth with $C^m(B,A)$
  \item Agg3 -- if $C^m(b,A)$ is undefined, smooth with the average of $C^{m'}(b,A)$ where $m'$ is defined
\end{itemize}

\subsubsection{Graphical models}

The Hierarchical Alignment Model (HAM) \citep{doyle_investigating_2016} defines alignment along $m$ as a linear effect on the log-odds of a reply containing $m$, estimated via Bayesian inference.

\[
  C^m(b,a) = logit^{-1}(P(\mathcal{E}^m_{u_b} \mid \mathcal{E}^m_{u_a})) - logit^{-1}(P(\mathcal{E}^m_{u_b} \mid \lnot \mathcal{E}^m_{u_a}))
\]

\bill{I couldn't find anything about the rationale for adding the no-$m$-in-$u_a$ conditional to the baseline estimation (subtractive component)} 

It seeks to improve on SCP in two ways:
\begin{enumerate}
  \item Consistency across marker frequencies -- SCP is sensitive to differences in (speaker independent) marker frequencies. This means it is not possible to compare SCP alignment scores across markers. HAM solves this problem by computing the alignment score in log-odds space.
  \item Robustness to sparse data -- SCP has only been applied to conversations with at least 10 messages \bill{(I don't think  \cite{danescu-niculescu-mizil_echoes_2012} says anything about this)}. HAM is designed to work with tweets where there might only be one reply pair between a given pair of users. The hierarchical prior on alignments (parametrized by marker and by dyad) assumes similar alignment matters across dyads for a given marker. 
\end{enumerate}

\cite{doyle_investigating_2016} Says the following about normalizing by message length:
``Due to the shortness of the messages, attempting to use marker counts normalized by message length could even introduce noise and weaken robustness to sparse data.'' but it seems this is exactly what the authors do with WHAM.

Variations:
\begin{itemize}
  \item Word-based HAM (WHAM) \citep{doyle_robust_2016} -- count proportion of $m$-tokens in $u_b$ (rather than binary presence). Corrects for ``artificial'' reply-length effects, but not the priming decay effect noted by Reitter et al (2006, 2008).
  \item Simplified WHAM (SWAM) \citep{shin_alignment_2018} -- removes the hierarchy of normal distributions in WHAM, which ``improve signal detection when group dynamics are subtle or group membership is difficult to determine''.  \bill{after looking at this paper again SWAM might actually be useful for us. The authors note that it is more computationally efficient than WHAM}
\end{itemize}



\subsubsection{Other approaches // criticism}
\begin{itemize}
  \item \cite{gao_understanding_2015} -- information theoretic approach that uses mutual information to measure dependence of $m$-usage in $u_b$ on $m$-usage in $u_a$. They also condition on message length and find that style coordination reported in other studies is partially attributed to message length coordination. \bill{It seems this method cannot distinguish between positive and negative coordination -- just degree of random variable dependence}.
  \item \cite{xu_not_2018} -- \bill{this is a false dichotomy?} low-level priming explains alignment better than sociolinguistic factors; message length confound; uses linear regression model
\end{itemize}

\subsection{Social network analysis}
\begin{itemize}
  \item \cite{blondel_fast_2008} -- Louvain clustering -- community detection method
  \item \cite{traag_louvain_2018} -- Leiden clustering -- improvement on Louvain method \bill{haven't tried this yet}
\end{itemize}

\subsection{Other}
\begin{itemize}
  \item \cite{hamilton_loyalty_2017} -- Reddit users ``employ language that signals collective identity''
  \item \cite{cocco_discourse_2012} -- text type clustering using PoS n-grams 
  \item \cite{hua_wikiconv:_2018} -- larger wiki talkpages corpus (including article talkpages), supposedly better preprocessing; also includes intermediate states (e.g., comments that were later deleted) \bill{we decided it's not worth using this over the original DNM talkpages corpus}
\end{itemize}


\section{New coordination metric(s)}

The goal is to combine the features we like from SCP:
\begin{itemize}
	\item easy to compute -- probabilities estimated directly instead of sampling
  \item possibility of per-utterance analysis (i.e., which specific utterances exemplify the most alignment?)
\end{itemize}
with those we like from WHAM:
\begin{itemize}
	\item word-based (so, response-length insensitive/normalized)
  \item universal (per-user) marker frequency baselines (why estimate a $b$'s baseline $m$-usage with only on $(a,b)$ pairs -- why not use all $b$-utterances?)
\end{itemize}

\subsection{Information theoretic metric}

Given a "reply pair" $(u_a, u_b)$ of utterances by speakers $a$ and $b$, where $u_b$ is an immediate reply to $u_b$, we define the coordination of utterance $u_b$ along marker $m$ in terms of the surprisal contained in the number of $m$-tokens appearing in $u_b$.

To measure surprisal, we assume that $b$ produces $m$-tokens according to a Bernouli distribution (i.e., bag of words assumption) estimated by $b$'s baseline $m$-token frequency, $f^m$, over all of their utterances.

The surprisal with respect to $m$ is then:
\[
	\log_2\big(Binom(c;f^m,l)\big) = \log_2\Big[\binom{len(u_b)}{c} f^{c}(1-f)^{l-c}\Big]
\]
where $c$ is the count of $m$-tokens in $u_b$, and $l$ is the length (in tokens) of $u_b$.

Given this surprisal score, we need to take into account two factors that affect its polarity with respect to coordination.
\begin{enumerate}
	\item whether the number of $m$-tokens in $u_b$ was higher or lower than the expected value based on $b$'s baseline, and
	\item whether $u_a$ contained any $m$-tokens
\end{enumerate}

\subsection{Word-based SCP}

Like SCP but except
\begin{enumerate}
  \item Like in WHAM, use $m$-frequency for $u_b$ rather than binary presence/absence (but still use binary $m$-presence for priming stimulus in $u_a$)
  \item Estimate $P(\mathcal{E}^m_{u_b})$ by summing across all of $b$'s utterances -- not just those in reply to $a$.
\end{enumerate}

\section{Experiments \& results}

\subsection{Descriptive statistics}
\begin{itemize}
  \item n users
  \item n posts
  \item years / posts per year
  \item n admins/n highly central (v. highly central) users
  \item avg. posts per user (+admins vs highly central)
  \item centrality histogram (w/admin colors) + two sample t-test centrality of admins/non-admins
\end{itemize}
\bill{todo}

\subsection{Replication/previous results}
\begin{itemize}
  \item coordination recieved (admin vs. highly central)
\end{itemize}
\bill{todo}


\subsection{Perplexity}
We looked at the perplexity of utterances for admin/non-admin and highly-central/not users. 
We measured perplexity with two models: KenLM \citep{heafield_kenlm:_2011}, and an RNN.
\bill{There was an earlier notebook \href{https://github.com/winobes/lasn/blob/68d536e15876d2a5178c354d2b5601170b8689cb/code/analysis.ipynb}{here} where the KenLM perplexity was higher users with social power and the effect size was greater for centrality than adminship. I think there may have been something wrong withe model because the ppl scores were much higher in general, but it could be worth taking another look at}

\begin{table}[H]
\centering
\begin{tabular}{@{}lllll@{}}
                     & \multicolumn{4}{c}{Perplexity (StdDev)} \\ \midrule
                     & RNN      &          & KenLM  &          \\ \midrule
(v.) high centrality & 149.648  & (43.72)  & 16.992 & (24.19)  \\
low centrality       & 149.996  & (157.71) & 17.511 & (53.92)  \\
admin                & 143.850  & (36.51)  & 15.456 & (22.52) \\
non-admin            & 151.244  & (151.60) & 17.805 & (52.17) 
\end{tabular}
\end{table}

Perplexity under the two models is negatively correlated.
The main cause appears to be that for some reason, KenLM assigned very high perplexity to short common utterances.
See \href{https://github.com/winobes/lasn/blob/43865d451ef925c1b0043ffa126e77449d26e36f/code/py_analysis.ipynb}{this notebook} for details.

A \href{https://github.com/winobes/lasn/blob/68d536e15876d2a5178c354d2b5601170b8689cb/code/analysis.ipynb}{previous look} at perplexity found that there is a weak negative association ($p < 0.001$, $r=-0.0130$) between perplexity and days-since-first-post. I.e., users' speech gets more regular over time.

\subsubsection{Trendiness}

For each year in the corpus, we train a language model $M^y$ and separately, a model $M$ for the entire corpus. We define the \emph{trendiness} of an utterance as the difference between its perplexity under the current-year model and the overall model:
\[
  T(u) = PPL(u) - PPL^y(u)
\]
where $u$ was uttered in year $y$.\bill{This could be refined with cleverer timeboxing of models}

$T(u)$ is positive if the utterance is more likely under the current-year model than the overall model.


Trendiness is positively correlated with Eigenvector centrality ($p < 0.01$). 
Likewise, binarized centrality has a weak but significant positive effect ($d = 0.029$, $p < 0.01$) on trendiness.\bill{These results were also obtained with the quesitonable KenLM model.}

\subsection{Linguistic style}
\begin{itemize}
  \item Multidimensional linguistic analysis (MDLA) -- \cite{biber_variation_1988}
  \item Applied to blogs -- \cite{grieve_variation_2011}
  \item Applied to Twitter -- \cite{clarke_dimensions_2017}
\end{itemize}
\subsubsection{Linguistic diversity (type-token ratio)}
Moving average type-token ratio \cite{covington_cutting_2010} attempts to solve the problem of mesasge length confound. We find that for both centrality and adminship, utterances from higher power users tend to exhibit greater linguistic diversity. Centrality \bill{v. high varriant} has a slightly larger effect size \href{https://github.com/winobes/lasn/blob/cf852ec9946123a182db004afec20fa30d1e4dfa/code/analysis.ipynb}{(notebook)}.

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
           & \multicolumn{2}{l}{mean MATTR-100} &           \\ \midrule
           & high power       & low power       & Cohen's D \\ \midrule
centrality & 0.6900           & 0.6809          & 0.1443    \\
adminship  & 0.6901           & 0.6826          & 0.1208    \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Formatting features}
\href{https://github.com/winobes/lasn/blob/51e511dd7ff6adf9d66fbf081def203566b1bce7/code/analysis.ipynb}{(notebook)}

\paragraph{High/low power:}

\begin{itemize}
  \item post length (in tokens)
    \begin{itemize}
      \item Non-admins have shorter posts than non-admins (3.08 vs. 3.23, $p < 0.01$)
      \item Low-centrality users have shorter posts than highly-central (3.08 vs. 3.36 $p < 0.01$)
    \end{itemize}
  \item italics
    \begin{itemize}
      \item Non-admins have a higher rate of italics usage than non-admins (0.31 vs. 0.15, $p<0.01$)
      \item Low-centrality users have a higher rate of italics usage than highly-central users (0.26 vs 0.11, $p<0.001$)
    \end{itemize}
  \item bold
    \begin{itemize}
      \item Non-admins have a higher rate of bold usage than admins (0.18 vs 0.10, $p<0.01$)
      \item Low-centrality users have higher rate of bold usage than high-centrality users (0.17 vs. 0.02, $p<0.01$)
    \end{itemize}
  \item links
    \begin{itemize}
      \item Non-admins have a higher rate of link usage than admins (0.14 vs 0.03, $p<0.01$)
      \item Low-centrality users have a higher rate of link usage than high-centrality users (0.11 vs 0.02, $p<0.01$)
    \end{itemize}
\end{itemize}

\paragraph{Between high power groups:}
\begin{itemize}
  \item the posts of highly-central users are not significantly longer than those of admins
  \item highly-central users use fewer URLs, italics, and boldface than admins ($p < 0.001$, effect size between 0.2 and 0.4)
  \item within the low centrality group, adminship does have an effect (admins use fewer italics, boldface, and links: $p < 0.05$, with effects between 0.2 and 0.5)
  \item centrality has a similar effect among non-admins
  \item with the (v.) highly-central group, adminship does not have an effect ($p > 0.05$ for any of the linguistic style features: length, italics, bold, links)
  \item among admins, highly-central users use significantly fewer italics, boldface, and links
\end{itemize}

\subsection{Sub-communities}
\href{https://github.com/winobes/lasn/blob/3a8dfcb1e48c56d07c0afe8973993c1c853d3908/code/analysis.ipynb}{notebook}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{louvain.png}
\caption{Sub-communities computed with \href{https://github.com/taynaud/python-louvain}{python-louvain}. Edges are weighted by number of links between community members. Nodes are scaled by size of community.}
\end{figure}

Users give/receive significantly less coordination to/from members of their own sub-community.

\bibliographystyle{humannat}
\bibliography{notes}

\end{document}
  
