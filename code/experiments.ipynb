{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "DELIM         = \" +++$+++ \"\n",
    "CONVO_FILE    = \"wikipedia.talkpages.conversations.txt\"\n",
    "USERS_FILE    = \"wikipedia.talkpages.userinfo.txt\"\n",
    "ADMINS_FILE   = \"wikipedia.talkpages.admins.txt\"\n",
    "POSTS_DF_FILE = \"posts_df.pickle\"\n",
    "USERS_DF_FILE = \"users_df.pickle\"\n",
    "POSTS_CSV     = \"posts.csv\"\n",
    "USERS_CSV     = \"users.csv\"\n",
    "NETWORK_FILE  = \"users_network.pickle\"\n",
    "CORPUS_DIR    = (\"../data/wiki/\")\n",
    "FWORDS_DIR    = '../data/function words/'\n",
    "MODEL_DIR     = CORPUS_DIR + 'rnn_lm/nov-20-full/'\n",
    "PPL_FILE      = MODEL_DIR + 'post_perplexities.csv'\n",
    "\n",
    "function_words = ['conjunctions', 'articles', 'prepositions', 'adverbs', 'quantifiers', \n",
    "           'impersonal_pronouns', 'personal_pronouns', 'auxiliary_verbs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataframes have been saved from a previous session, they can be loaded with this cell instead of running the cells that create them. Individual cells below can be run to re-compute columns in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(CORPUS_DIR + POSTS_DF_FILE):\n",
    "    posts = pd.read_pickle(CORPUS_DIR + POSTS_DF_FILE)\n",
    "if os.path.isfile(CORPUS_DIR + USERS_DF_FILE):\n",
    "    users = pd.read_pickle(CORPUS_DIR + USERS_DF_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posts\n",
    "\n",
    "## Create the `posts` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['utterance_id', 'user', 'talkpage_user', 'conversation_root', 'reply_to', \n",
    "           'timestamp', 'timestamp_unixtime', 'clean_text', 'raw_text']\n",
    "\n",
    "posts = {column: [] for column in columns}\n",
    "with open(CORPUS_DIR + CONVO_FILE) as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        \n",
    "        # parse lines from the conversations file\n",
    "        if line.startswith(\"could not match\") or line.strip() == \"\":\n",
    "            continue\n",
    "        line = line.rstrip('\\n').split(DELIM)\n",
    "        assert(len(line) == len(columns))\n",
    "        line = {column: value for column, value in zip(columns, line)}\n",
    "        \n",
    "        # convert timestamps to datetime objects\n",
    "        try:\n",
    "            line['timestamp'] = datetime.strptime(line['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            line['timestamp'] = None\n",
    "            \n",
    "        for column, value in line.items():\n",
    "            posts[column].append(value)\n",
    "\n",
    "            \n",
    "posts = pd.DataFrame(data=posts, index=posts['utterance_id'], columns=columns, dtype=str)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.tokenize.word_tokenize(text) for text in tqdm(posts['clean_text'])]\n",
    "posts = posts.assign(tokens=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect formatting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "bold = [False] * len(posts)\n",
    "italics = [False] * len(posts)\n",
    "links = [False] * len(posts)\n",
    "\n",
    "for i, text in enumerate(tqdm(posts['clean_text'])):\n",
    "    \n",
    "    if re.search(\"'''''.+'''''\", text):\n",
    "        bold[i] = True\n",
    "        italics[i] = True\n",
    "    else:\n",
    "        if re.search(\"'''.+'''\", text):\n",
    "            bold[i] = True\n",
    "        if re.search(\"''.+''\", text):\n",
    "            italics[i] = True\n",
    "            \n",
    "    if re.search(\"\\[\\[.+\\]\\]\", text):\n",
    "        links[i] = True\n",
    "        \n",
    "posts = posts.assign(bold=bold, italics=italics, links=links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect style markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {feature: [] for feature in function_words}\n",
    "for feature in function_words:\n",
    "    with open(FWORDS_DIR + feature + '.txt') as f:\n",
    "        markers[feature] = [word.rstrip('\\n') for word in f.readlines()]\n",
    "        \n",
    "feature_columns = {m: [False] * len(posts) for m in function_words}       \n",
    "for i, tokens in enumerate(tqdm(posts['tokens'])):\n",
    "    for m in function_words:\n",
    "        if any(t.lower() in markers[m] for t in tokens):\n",
    "            feature_columns[m][i] = True\n",
    "        \n",
    "posts = posts.assign(**feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record length (in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['length'] = posts['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rnn_lm import Evaluator\n",
    "\n",
    "if os.path.isfile(PPL_FILE): # load from previously computed file\n",
    "    perplexity = pd.read_csv(PPL_FILE, dtype={'utterance_id': object, 'perplexity': np.float64})\n",
    "    perplexity.set_index('utterance_id', inplace=True)\n",
    "    posts = pd.merge(posts, perplexity, how='left')\n",
    "\n",
    "else: # compute with the RNN model\n",
    "    evaluator = Evaluator(MODEL_DIR)\n",
    "    perplexity = []\n",
    "    for i, tokens in enumerate(tqdm(posts['tokens'])):\n",
    "        if not tokens: # if the post is empty \n",
    "            perplexity.append(np.nan)\n",
    "        else:\n",
    "            perplexity.append(evaluator.perplexity(tokens))\n",
    "\n",
    "    posts = posts.assign(perplexity=perplexity)\n",
    "    posts.to_csv(PPL_FILE, columns=['utterance_id', 'perplexity'])  # save file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user', 'edit_count', 'gender', 'numerical_id']\n",
    "\n",
    "users = {column: [] for column in columns}\n",
    "with open(CORPUS_DIR + USERS_FILE) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.rstrip('\\n').split(DELIM)\n",
    "        assert(len(line) == len(columns))\n",
    "        line = {column: value for column, value in zip(columns, line)}\n",
    "        for column, value in line.items():\n",
    "            users[column].append(value)\n",
    "            \n",
    "users = pd.DataFrame(data=users, index=users['user'], columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user', 'admin_ascension']\n",
    "\n",
    "admins = {column: [] for column in columns}\n",
    "with open(CORPUS_DIR + ADMINS_FILE) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.rstrip('\\n').split(' ')\n",
    "        line = ' '.join(line[:-1]), line[-1]\n",
    "        assert(len(line) == len(columns))\n",
    "        line = {column: value for column, value in zip(columns, line)}\n",
    "            \n",
    "        # convert timestamps to datetime objects\n",
    "        try:\n",
    "            line['admin_ascension'] = datetime.strptime(line['admin_ascension'], \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            line['admin_ascension'] = None\n",
    "            \n",
    "        for column, value in line.items():\n",
    "            admins[column].append(value)\n",
    "            \n",
    "admins = pd.DataFrame(data=admins, index=admins['user'], columns=columns)\n",
    "users = pd.merge(users, admins, on='user', how='left').set_index('user')\n",
    "users['admin'] = users['admin_ascension'].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average post features by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# most of the post features are binary (-> frequencies) but these two are averages\n",
    "avg_cols = {'perplexity_freq': 'perplexity_avg', 'length_freq': 'length_avg'}\n",
    "users = users.join(posts.groupby('user').mean().add_suffix('_freq').rename(columns=avg_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "network = nx.Graph()\n",
    "\n",
    "pairs = pd.merge(posts, posts, how='inner', left_on='reply_to', right_index=True, suffixes=['_b', '_a'])\n",
    "for i, pair in tqdm(pairs.iterrows(), total=len(pairs)):\n",
    "    user_a, user_b = pair['user_a'], pair['user_b']\n",
    "    if user_a == user_b or ' ' in (user_a, user_b):\n",
    "        continue  # ignore self-talk\n",
    "    elif network.has_edge(user_a, user_b):\n",
    "        network[user_a][user_b]['weight'] += 1\n",
    "    else:\n",
    "        network.add_edge(user_a, user_b, weight=1)\n",
    "        \n",
    "print(\"The unpruned network has {} notes (users).\".format(len(network.nodes())))\n",
    "\n",
    "# pruning the network to it's largest component:\n",
    "minor_components = list(nx.connected_components(network))[1:]\n",
    "disconnected_users = [user for com in minor_components for user in com]\n",
    "network.remove_nodes_from(disconnected_users)\n",
    "print(\"Removed {} users from {} disconnected components.\".format(len(disconnected_users), len(minor_components)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_centrality = nx.eigenvector_centrality_numpy(network)\n",
    "eigen_centrality = pd.DataFrame(list(eigen_centrality.items()), columns=['user', 'eigen_central'])\n",
    "eigen_centrality.set_index('user', inplace=True)\n",
    "users = users.reset_index().join(eigen_centrality).set_index('user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize centrality\n",
    "\n",
    "We binarize eigenvector centrality (highly central vs. not highly central) by taking those users whose cenrality is one standard deviation above the mean.\n",
    "\n",
    "TODO: this is a problematic binarization technique becuase centrality is not normaly distributed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['eigen_central_bin'] = users['eigen_central'] > users['eigen_central'].mean() + users['eigen_central'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Louvian communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community # https://github.com/taynaud/python-louvain\n",
    "\n",
    "partition = community.best_partition(network)\n",
    "partition = pd.DataFrame(list(partition.items()), columns=['user', 'community'])\n",
    "partition.set_index('user', inplace=True)\n",
    "users = users.join(partition, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "\n",
    "partition = users[pd.notna(users.community)].community.astype(int)\n",
    "clusters = nx.Graph()\n",
    "clusters.add_nodes_from(partition.unique(), weight=0)\n",
    "\n",
    "for user in network.nodes():\n",
    "    if user in partition:\n",
    "        clusters.node[partition.loc[user]]['weight'] += 1\n",
    "\n",
    "for (u, v), weight in nx.get_edge_attributes(network, 'weight').items():\n",
    "    if u in partition and v in partition:\n",
    "        u, v = partition[u], partition[v]\n",
    "        if not clusters.has_edge(u, v):\n",
    "            clusters.add_edge(u, v, weight=weight)\n",
    "        else:\n",
    "            clusters[u][v]['weight'] += weight\n",
    "        \n",
    "nodes = nx.get_node_attributes(clusters, 'weight').items()\n",
    "edges = nx.get_edge_attributes(clusters, 'weight').items()\n",
    "\n",
    "nodes, node_weights = zip(*nodes)\n",
    "edges, edge_weights = zip(*edges)\n",
    "\n",
    "node_weights = [w / max(node_weights) * 300 for w in node_weights]\n",
    "edge_weights = [w / max(edge_weights) * 10 for w in edge_weights]\n",
    "\n",
    "pos = nx.random_layout(clusters)\n",
    "nx.draw_networkx_nodes(clusters, pos, nodes, node_size=node_weights)\n",
    "nx.draw_networkx_labels(clusters, pos)\n",
    "nx.draw_networkx_edges(clusters, pos, edgelist=edges, width=edge_weights)\n",
    "\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Coordination\n",
    "\n",
    "### Coordination given\n",
    "\n",
    "For a user $b$ and a group of users $A$, let $S^A_b$ be the set of pairs of utterances $(u_a, u_b)$ where $u_b$ is utterd by $b$ in reply to the parent utterance $u_a$, uttered by $a \\in A$ \n",
    "\n",
    "$\\mathcal{E}_m(u)$ means that utterance $u$ exhibits some linguistic marker, $m$.\n",
    "\n",
    "Following *Echoes of Power* we define the coordination of user $b$ towards a group $A$ (the *coordination given* by $b$) as follows:\n",
    "$$\n",
    "C^g_m(A,b) = P\\big[\\mathcal{E}_m(u_b) \\mid \\mathcal{E}_m(u_a) \\land (u_a, u_b) \\in S^A_b\\big] -\n",
    "P\\big[\\mathcal{E}_m(u_b) \\mid (u_a, u_b) \\in S^A_b\\big]\n",
    "$$\n",
    "\n",
    "The probabilities are estimated by counting occurances of $m$ in $S^A_b$:\n",
    "\n",
    "$$\n",
    "C^g_m(A,b) \\approx \\sum_{(u_a,u_b)\\in S^A_b}\\Big({\n",
    "\\frac{[\\mathcal{E}_m(u_a) \\land \\mathcal{E}_m(u_b)]}{[\\mathcal{E}_m(u_a)]} - \n",
    "\\frac{[\\mathcal{E}_m(u_b)]}{1}}  \\Big)\n",
    "$$\n",
    "\n",
    "$C^m(A,b)$ is defined for $m$, $b$ and $A$ where $b$ where $\\{(u_a, u_b) \\in S^A_b \\mid \\mathcal{E}_m(u_a)\\} \\neq \\varnothing $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord(pairs, prefix):\n",
    "\n",
    "    pairs = pairs.assign(count=[1] * len(pairs))  # for summing later\n",
    "\n",
    "    # for each marker, add column indicating if both parent & reply exhibits m\n",
    "    both_exhibit_m = {}\n",
    "    for m in function_words:\n",
    "        both_exhibit_m[m + '_both'] = pairs[m + '_b'] & pairs[m + '_A']\n",
    "    pairs = pairs.assign(**both_exhibit_m)\n",
    "\n",
    "    total = pairs.groupby('user_b').sum()\n",
    "\n",
    "    coord = {}\n",
    "    for m in function_words:\n",
    "        coord[prefix + m] = total[m + '_both'] / total[m + '_b'] - total[m + '_A'] / total['count']\n",
    "    coord = pd.DataFrame(coord) \n",
    "    coord[prefix + 'aggrigate'] = coord.mean(axis=1, skipna=True)\n",
    "    \n",
    "    return coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join posts with their parent posts (where b is responding to a in A)\n",
    "pairs = pd.merge(posts, posts, how='inner', left_index=True, right_on='reply_to', suffixes=['_A', '_b'])\n",
    "coord_given = get_coord(pairs, 'coord_given_')\n",
    "\n",
    "users = users.join(coord_given, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordination Received\n",
    "\n",
    "Likewise, we estimate the coordination of a group $A$ towards a user $b$ (the *coordination received* by $b$) as:\n",
    "\n",
    "$$\n",
    "C^r_m(A,b) \\approx \\sum_{(u_b,u_a)\\in S^b_A}\\Big({\n",
    "\\frac{[\\mathcal{E}_m(u_b) \\land \\mathcal{E}_m(u_a)]}{[\\mathcal{E}_m(u_b)]} - \n",
    "\\frac{[\\mathcal{E}_m(u_a)]}{1}}  \\Big)\n",
    "$$\n",
    "\n",
    "where $S^b_A$ is the set of pairs of utterances where a member of group $A$ is replying to an utteance of user $b$ (note that this is an entirely distinct set from $S^A_b$).\n",
    "\n",
    "As before, $C^r_m(A,b)$ is defined if $\\{(u_b, u_a) \\in S^b_A \\mid \\mathcal{E}_m(u_b)\\} \\neq \\varnothing $\n",
    "\n",
    "In both cases, to aggregate over markers, we take the average of the marker-specific coordination measures for which $C^*_m(A,b)$ is defined.\n",
    "\n",
    "First, we calculate each user's coordination (given and received) with respect to the general population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join posts with their parent posts (where a in A is responding to b)\n",
    "pairs = pd.merge(posts, posts, how='inner', left_index=True, right_on='reply_to', suffixes=['_b', '_A'])\n",
    "coord_received = get_coord(pairs, 'coord_received_')\n",
    "\n",
    "users = users.join(coord_received, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordination by community\n",
    "\n",
    "Now we want to see how users coordinate (and are coordinated with) by users inside and outside of their own Louvian communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.merge(posts, posts, how='inner', left_index=True, right_on='reply_to', suffixes=['_A', '_b'])\n",
    "# merege with `users` to get the Louvian community\n",
    "pairs = pd.merge(pairs, users[['community']], left_on='user_A', right_index=True).rename(columns={'community': 'community_A'})\n",
    "pairs = pd.merge(pairs, users[['community']], left_on='user_b', right_index=True).rename(columns={'community': 'community_b'})\n",
    "\n",
    "# split the reply pairs by whether the users belong to the same community or not\n",
    "coord_given_ingroup = get_coord(pairs[pairs.community_A == pairs.community_b], 'ingroup_coord_given_')\n",
    "coord_given_outgroup = get_coord(pairs[pairs.community_A != pairs.community_b], 'outgroup_coord_given_')\n",
    "\n",
    "# only record the aggrigate for now.. may be too much otherwise\n",
    "users = users.join(coord_given_ingroup[['ingroup_coord_given_aggrigate']], how='left')\n",
    "users = users.join(coord_given_outgroup[['outgroup_coord_given_aggrigate']], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing for coordination received \n",
    "\n",
    "pairs = pd.merge(posts, posts, how='inner', left_index=True, right_on='reply_to', suffixes=['_b', '_A'])\n",
    "# merege with `users` to get the Louvian community\n",
    "pairs = pd.merge(pairs, users[['community']], left_on='user_A', right_index=True).rename(columns={'community': 'community_A'})\n",
    "pairs = pd.merge(pairs, users[['community']], left_on='user_b', right_index=True).rename(columns={'community': 'community_b'})\n",
    "\n",
    "# split the reply pairs by whether the users belong to the same community or not\n",
    "coord_given_ingroup = get_coord(pairs[pairs.community_A == pairs.community_b], 'ingroup_coord_received_')\n",
    "coord_given_outgroup = get_coord(pairs[pairs.community_A != pairs.community_b], 'outgroup_coord_received_')\n",
    "\n",
    "# only record the aggrigate for now.. may be too much otherwise\n",
    "users = users.join(coord_given_ingroup[['ingroup_coord_received_aggrigate']], how='left')\n",
    "users = users.join(coord_given_outgroup[['outgroup_coord_received_aggrigate']], how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_pickle(CORPUS_DIR + POSTS_DF_FILE)\n",
    "users.to_pickle(CORPUS_DIR + USERS_DF_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv(CORPUS_DIR + POSTS_CSV)\n",
    "users.to_csv(CORPUS_DIR + USERS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
