{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening pickle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26397/26397 [00:00<00:00, 314181.81it/s]\n",
      "  6%|▌         | 21942/389121 [00:00<00:01, 219362.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading users...\n",
      "Loading posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389121/389121 [00:02<00:00, 142320.68it/s]\n",
      "100%|██████████| 26397/26397 [00:00<00:00, 260062.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening corpus files...\n",
      "Loading users...\n",
      "Loading posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 516766/516766 [00:13<00:00, 37529.88it/s]\n",
      "  0%|          | 43/389121 [00:00<15:05, 429.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389121/389121 [06:44<00:00, 960.92it/s] \n",
      "  3%|▎         | 13541/389121 [00:00<00:02, 135378.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating network ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389121/389121 [00:01<00:00, 211717.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 30356 posts to/from unknown users.\n",
      "The unpruned network has  22495 nodes.\n",
      "Pruning network to its largest component...\n",
      "\t removed 398 users from 192 disconnected components.\n"
     ]
    }
   ],
   "source": [
    "from wiki import WikiCorpus, PICKLE_FILE, CORPUS_DIR\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    corpus = WikiCorpus.from_pickle(PICKLE_FILE)\n",
    "except:\n",
    "    corpus = WikiCorpus.from_corpus_files()\n",
    "    corpus.generate_network('all_users', normalize_edge_weights=False)\n",
    "    corpus.to_pickle(PICKLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.5, 'clip': 0.25, 'sample': None, 'num_layers': 2, 'job': 'train', 'max_seq_len': 32, 'job_dir': 'nov-20-full', 'batch_size': 1024, 'embedding_dim': 200, 'initial_learning_rate': 20, 'hidden_dim': 200, 'max_tokens': 10000, 'epochs': 40}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 21802/389121 [09:51<2:46:10, 36.84it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fd01f209494a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpost_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mppl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpost_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_post_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'perplexity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lasn/code/rnn_lm.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model, tokenizer, tokens)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lasn/env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lasn/env/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 482\u001b[0;31m                                self.ignore_index)\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lasn/env/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maveraged\u001b[0m \u001b[0mover\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mignored\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \"\"\"\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lasn/env/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lasn/env/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, *params)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import rnn_lm\n",
    "\n",
    "model_dir = '../data/wiki/rnn_lm/nov-20-full/'\n",
    "\n",
    "tokenizer = rnn_lm.Tokenizer.load(model_dir)\n",
    "model = rnn_lm.LSTM.load(tokenizer, model_dir)\n",
    "\n",
    "ppl = {}\n",
    "for post_id in tqdm(corpus.posts):\n",
    "    tokens = [token for sentence in corpus.posts[post_id].tokens for token in sentence]\n",
    "    if len(tokens) > 1:\n",
    "        ppl[post_id] = rnn_lm.perplexity(model, tokenizer, tokens)\n",
    "corpus.register_post_data('perplexity', ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/389121 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▎         | 14355/389121 [00:00<00:02, 142312.71it/s]\u001b[A\n",
      "  8%|▊         | 31170/389121 [00:00<00:02, 155303.81it/s]\u001b[A\n",
      " 12%|█▏        | 48236/389121 [00:00<00:02, 160408.20it/s]\u001b[A\n",
      " 16%|█▌        | 63160/389121 [00:00<00:02, 157549.03it/s]\u001b[A\n",
      " 20%|█▉        | 77174/389121 [00:00<00:02, 154100.00it/s]\u001b[A\n",
      " 24%|██▎       | 91563/389121 [00:00<00:01, 152325.20it/s]\u001b[A\n",
      " 28%|██▊       | 109615/389121 [00:00<00:01, 156270.51it/s]\u001b[A\n",
      " 33%|███▎      | 127916/389121 [00:00<00:01, 159748.09it/s]\u001b[A\n",
      " 38%|███▊      | 146656/389121 [00:00<00:01, 162816.61it/s]\u001b[A\n",
      " 43%|████▎     | 165956/389121 [00:01<00:01, 165642.00it/s]\u001b[A\n",
      " 47%|████▋     | 183385/389121 [00:01<00:01, 166262.76it/s]\u001b[A\n",
      " 52%|█████▏    | 201791/389121 [00:01<00:01, 167750.58it/s]\u001b[A\n",
      " 56%|█████▋    | 219843/389121 [00:01<00:01, 168711.17it/s]\u001b[A\n",
      " 61%|██████▏   | 238435/389121 [00:01<00:00, 169973.02it/s]\u001b[A\n",
      " 66%|██████▌   | 257237/389121 [00:01<00:00, 171173.33it/s]\u001b[A\n",
      " 71%|███████   | 276526/389121 [00:01<00:00, 172508.49it/s]\u001b[A\n",
      " 76%|███████▌  | 296619/389121 [00:01<00:00, 174126.95it/s]\u001b[A\n",
      " 81%|████████  | 316038/389121 [00:01<00:00, 175247.31it/s]\u001b[A\n",
      " 86%|████████▌ | 335179/389121 [00:01<00:00, 175758.27it/s]\u001b[A\n",
      " 91%|█████████ | 354142/389121 [00:02<00:00, 174869.76it/s]\u001b[A\n",
      " 96%|█████████▌| 372194/389121 [00:02<00:00, 174520.54it/s]\u001b[A\n",
      "100%|██████████| 389121/389121 [00:02<00:00, 175012.60it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "len(ppl)\n",
    "post_len = {}\n",
    "for post_id in tqdm(corpus.posts):\n",
    "    tokens = [token for sentence in corpus.posts[post_id].tokens for token in sentence]\n",
    "    if len(tokens) > 1:\n",
    "        post_len[post_id] = len(tokens)\n",
    "corpus.register_post_data('perplexity', ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/389121 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 402/389121 [00:00<01:37, 3981.79it/s]\u001b[A\n",
      "  0%|          | 833/389121 [00:00<01:33, 4134.04it/s]\u001b[A\n",
      "  0%|          | 1240/389121 [00:00<01:34, 4111.14it/s]\u001b[A\n",
      "  0%|          | 1644/389121 [00:00<01:34, 4091.50it/s]\u001b[A\n",
      "  1%|          | 2048/389121 [00:00<01:34, 4079.68it/s]\u001b[A\n",
      "  1%|          | 2434/389121 [00:00<01:35, 4043.16it/s]\u001b[A\n",
      "  1%|          | 2873/389121 [00:00<01:34, 4091.32it/s]\u001b[A\n",
      "  1%|          | 3294/389121 [00:00<01:34, 4103.72it/s]\u001b[A\n",
      "  1%|          | 3718/389121 [00:00<01:33, 4116.70it/s]\u001b[A\n",
      "  1%|          | 4140/389121 [00:01<01:33, 4123.31it/s]\u001b[A\n",
      "  1%|          | 4547/389121 [00:01<01:35, 4025.69it/s]\u001b[A\n",
      "  1%|▏         | 4929/389121 [00:01<01:37, 3942.82it/s]\u001b[A\n",
      "  1%|▏         | 5291/389121 [00:01<01:38, 3893.91it/s]\u001b[A\n",
      "  1%|▏         | 5683/389121 [00:01<01:38, 3895.57it/s]\u001b[A\n",
      "  2%|▏         | 6101/389121 [00:01<01:37, 3913.16it/s]\u001b[A\n",
      "  2%|▏         | 6483/389121 [00:01<01:37, 3906.81it/s]\u001b[A\n",
      "  2%|▏         | 6870/389121 [00:01<01:37, 3904.19it/s]\u001b[A\n",
      "  2%|▏         | 7265/389121 [00:01<01:37, 3905.96it/s]\u001b[A\n",
      "  2%|▏         | 7651/389121 [00:01<01:37, 3901.38it/s]\u001b[A\n",
      "  2%|▏         | 8036/389121 [00:02<01:38, 3874.28it/s]\u001b[A\n",
      "  2%|▏         | 8440/389121 [00:02<01:38, 3880.46it/s]\u001b[A\n",
      "  2%|▏         | 8853/389121 [00:02<01:37, 3891.70it/s]\u001b[A\n",
      "  2%|▏         | 9274/389121 [00:02<01:37, 3905.01it/s]\u001b[A\n",
      "  2%|▏         | 9674/389121 [00:02<01:37, 3907.09it/s]\u001b[A\n",
      "  3%|▎         | 10099/389121 [00:02<01:36, 3919.67it/s]\u001b[A\n",
      "  3%|▎         | 10521/389121 [00:02<01:36, 3930.95it/s]\u001b[A\n",
      "  3%|▎         | 10954/389121 [00:02<01:35, 3944.75it/s]\u001b[A\n",
      "  3%|▎         | 11371/389121 [00:02<01:35, 3946.62it/s]\u001b[A\n",
      "  3%|▎         | 11793/389121 [00:02<01:35, 3954.93it/s]\u001b[A\n",
      "  3%|▎         | 12207/389121 [00:03<01:35, 3957.66it/s]\u001b[A\n",
      "  3%|▎         | 12642/389121 [00:03<01:34, 3968.84it/s]\u001b[A\n",
      "  3%|▎         | 13060/389121 [00:03<01:34, 3971.32it/s]\u001b[A\n",
      "  3%|▎         | 13485/389121 [00:03<01:34, 3978.85it/s]\u001b[A\n",
      "  4%|▎         | 13902/389121 [00:03<01:34, 3981.99it/s]\u001b[A\n",
      "  4%|▎         | 14316/389121 [00:03<01:34, 3976.67it/s]\u001b[A\n",
      "  4%|▍         | 14724/389121 [00:03<01:34, 3979.42it/s]\u001b[A\n",
      "  4%|▍         | 15135/389121 [00:03<01:33, 3982.91it/s]\u001b[A\n",
      "  4%|▍         | 15543/389121 [00:03<01:33, 3984.33it/s]\u001b[A\n",
      "  4%|▍         | 15950/389121 [00:04<01:33, 3984.86it/s]\u001b[A\n",
      "  4%|▍         | 16365/389121 [00:04<01:33, 3988.73it/s]\u001b[A\n",
      "  4%|▍         | 16773/389121 [00:04<01:34, 3959.35it/s]\u001b[A\n",
      "  4%|▍         | 17150/389121 [00:04<01:35, 3897.80it/s]\u001b[A\n",
      "  4%|▍         | 17484/389121 [00:04<01:36, 3853.40it/s]\u001b[A\n",
      "  5%|▍         | 17790/389121 [00:04<01:37, 3801.63it/s]\u001b[A\n",
      "  5%|▍         | 18069/389121 [00:04<01:38, 3765.45it/s]\u001b[A\n",
      "  5%|▍         | 18586/389121 [00:05<01:40, 3679.56it/s]Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/winobes/lasn/env/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/winobes/lasn/env/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 389121/389121 [01:40<00:00, 3860.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from corpus import markers\n",
    "\n",
    "post_align = {}\n",
    "for post in tqdm(corpus.posts.values()):\n",
    "    post_align[post.id] = False\n",
    "    parent = post.get_parent()\n",
    "    if not parent:\n",
    "        continue\n",
    "    for marker in markers:\n",
    "        if parent.exhibits_marker(marker) and post.exhibits_marker(marker):\n",
    "            if not parent.id in post_align:\n",
    "                post_align[parent.id] = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "header = ['post_id', 'len', 'ppl', 'align']\n",
    "with open(CORPUS_DIR + 'post_data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for post in corpus.posts.values():\n",
    "        data = [post.id, post_len.get(post.id, None), ppl.get(post.id, None), post_align.get(post.id, None)]\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhibits Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import markers\n",
    "\n",
    "for maker in markers:\n",
    "    exhibits_dir = {post.exhibits_marker}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Features\n",
    "\n",
    "### Eigenvector centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy\n",
    "\n",
    "eigen_central = nx.eigenvector_centrality_numpy(corpus.networks['all_users'])\n",
    "corpus.register_user_data('eigen_central', eigen_central)\n",
    "\n",
    "mean = numpy.mean(list(eigen_central.values()))\n",
    "stddev = numpy.std(list(eigen_central.values()))\n",
    "eigen_central_bin = {user: e > mean + stddev for user, e in eigen_central.items()}\n",
    "corpus.register_user_data('eigen_central_bin', eigen_central_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community clustering (Louvain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import community # https://github.com/taynaud/python-louvain\n",
    "\n",
    "partition = community.best_partition(corpus.networks['all_users'])\n",
    "corpus.register_user_data('community', partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Wikipedia network by considering community cluster as its own node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "\n",
    "clusters = nx.Graph()\n",
    "clusters.add_nodes_from(set(partition.values()), weight=0)\n",
    "\n",
    "for user in corpus.networks['all_users'].nodes():\n",
    "    clusters.node[partition[user]]['weight'] += 1\n",
    "\n",
    "for (u, v), weight in nx.get_edge_attributes(corpus.networks['all_users'], 'weight').items():\n",
    "    u, v = partition[u], partition[v]\n",
    "    if not clusters.has_edge(u, v):\n",
    "        clusters.add_edge(u, v, weight=weight)\n",
    "    else:\n",
    "        clusters.edge[u][v]['weight'] += weight\n",
    "        \n",
    "nodes = nx.get_node_attributes(clusters, 'weight').items()\n",
    "edges = nx.get_edge_attributes(clusters, 'weight').items()\n",
    "\n",
    "nodes, node_weights = zip(*nodes)\n",
    "edges, edge_weights = zip(*edges)\n",
    "\n",
    "node_weights = [w / max(node_weights) * 300 for w in node_weights]\n",
    "edge_weights = [w / max(edge_weights) * 10 for w in edge_weights]\n",
    "\n",
    "pos = nx.random_layout(clusters)\n",
    "nx.draw_networkx_nodes(clusters, pos, nodes, node_size=node_weights)\n",
    "nx.draw_networkx_labels(clusters, pos)\n",
    "nx.draw_networkx_edges(clusters, pos, edgelist=edges, width=edge_weights)\n",
    "\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Coordination\n",
    "\n",
    "For a user $b$ and a group of users $A$, let $S^A_b$ be the set of pairs of utterances $(u_a, u_b)$ where $u_b$ is utterd by $b$ in reply to the parent utterance $u_a$, uttered by $a \\in A$ \n",
    "\n",
    "$\\mathcal{E}_m(u)$ means that utterance $u$ exhibits some linguistic marker, $m$.\n",
    "\n",
    "Following *Echoes of Power* we define the coordination of user $b$ towards a group $A$ (the *coordination given* by $b$) as follows:\n",
    "$$\n",
    "C^g_m(A,b) = P\\big[\\mathcal{E}_m(u_b) \\mid \\mathcal{E}_m(u_a) \\land (u_a, u_b) \\in S^A_b\\big] -\n",
    "P\\big[\\mathcal{E}_m(u_b) \\mid (u_a, u_b) \\in S^A_b\\big]\n",
    "$$\n",
    "\n",
    "The probabilities are estimated by counting occurances of $m$ in $S^A_b$:\n",
    "\n",
    "$$\n",
    "C^g_m(A,b) \\approx \\sum_{(u_a,u_b)\\in S^A_b}\\Big({\n",
    "\\frac{[\\mathcal{E}_m(u_a) \\land \\mathcal{E}_m(u_b)]}{[\\mathcal{E}_m(u_a)]} - \n",
    "\\frac{[\\mathcal{E}_m(u_b)]}{1}}  \\Big)\n",
    "$$\n",
    "\n",
    "$C^m(A,b)$ is defined for $m$, $b$ and $A$ where $b$ where $\\{(u_a, u_b) \\in S^A_b \\mid \\mathcal{E}_m(u_a)\\} \\neq \\varnothing $.\n",
    "\n",
    "Likewise, we estimate the coordination of a group $A$ towards a user $b$ (the *coordination received* by $b$) as:\n",
    "\n",
    "$$\n",
    "C^r_m(A,b) \\approx \\sum_{(u_b,u_a)\\in S^b_A}\\Big({\n",
    "\\frac{[\\mathcal{E}_m(u_b) \\land \\mathcal{E}_m(u_a)]}{[\\mathcal{E}_m(u_b)]} - \n",
    "\\frac{[\\mathcal{E}_m(u_a)]}{1}}  \\Big)\n",
    "$$\n",
    "\n",
    "where $S^b_A$ is the set of pairs of utterances where a member of group $A$ is replying to an utteance of user $b$ (note that this is an entirely distinct set from $S^A_b$).\n",
    "\n",
    "As before, $C^r_m(A,b)$ is defined if $\\{(u_b, u_a) \\in S^b_A \\mid \\mathcal{E}_m(u_b)\\} \\neq \\varnothing $\n",
    "\n",
    "In both cases, to aggregate over markers, we take the average of the marker-specific coordination measures for which $C^*_m(A,b)$ is defined.\n",
    "\n",
    "First, we calculate each user's coordination (given and received) with respect to the general population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_given, coord_received  = corpus.get_coordination()\n",
    "corpus.register_user_data('coord_given_all', coord_given['agg3'])\n",
    "corpus.register_user_data('coord_received_all', coord_received['agg3'])\n",
    "# we could easily also register the per-marker coordination measures here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate users' coordination with respect to their Louvain sub-group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_communities = max(user.data['community'] if user.data['community'] else -1 for user in corpus.users.values())\n",
    "ingroup_coord_given, ingroup_coord_received = {}, {}\n",
    "for community_id in range(n_communities):\n",
    "    ingroup = [user.id for user in corpus.users.values() if user.data['community'] == community_id]\n",
    "    print('Calculating coordination for community {} ({} people)...'.format(community_id, len(ingroup)))\n",
    "    coord_given, coord_received = corpus.get_coordination(ingroup, ingroup)\n",
    "    ingroup_coord_given.update(coord_given)\n",
    "    ingroup_coord_received.update(coord_received)\n",
    "    # could also calculate for out-group coordination\n",
    "corpus.register_user_data('coord_given_ingroup', ingroup_coord_given['agg3'])\n",
    "corpus.register_user_data('coord_received_ingroup', ingroup_coord_received['agg3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistc Style Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from corpus import markers\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "post_count = Counter()\n",
    "italics_count = Counter()\n",
    "bold_count = Counter()\n",
    "link_count = Counter()\n",
    "function_words_count = Counter()\n",
    "total_tokens = Counter()\n",
    "\n",
    "for post in corpus.posts.values():  \n",
    "    user = post.author_id\n",
    "    post_count[user] += 1\n",
    "    if re.search(\"''''.+''''\", post.clean_text):\n",
    "        bold_count[user] += 1\n",
    "        italics_count[user] += 1\n",
    "    else:\n",
    "        if re.search(\"'''.+'''''\", post.clean_text):\n",
    "            bold_count[user] += 1\n",
    "        if re.search(\"''.+''\", post.clean_text):\n",
    "            italics_count[user] += 1           \n",
    "    if re.search(\"[[.+]]\", post.clean_text):\n",
    "        link_count[user] += 1\n",
    "    for t in post.get_tokens():\n",
    "        if any(t.lower() in markers[m] for m in markers):\n",
    "            function_words_count[user] += 1\n",
    "    total_tokens[user] += len(post.tokens)\n",
    "\n",
    "def per_post(item_count):\n",
    "    return {user: item_count[user] / post_count[user] if post_count[user] else None for user in item_count}\n",
    "\n",
    "corpus.register_user_data('post_count', post_count)\n",
    "corpus.register_user_data('italics_freq', per_post(italics_count))\n",
    "corpus.register_user_data('bold_freq', per_post(bold_count))\n",
    "corpus.register_user_data('link_freq', per_post(link_count))\n",
    "corpus.register_user_data('function_words_freq', per_post(function_words_count))\n",
    "corpus.register_user_data('avg_length_tokens', per_post(total_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.to_pickle(PICKLE_FILE)\n",
    "corpus.export_user_data(CORPUS_DIR + 'user_data.csv', blacklist = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
