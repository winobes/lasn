{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "DELIM         = \" +++$+++ \"\n",
    "CONVO_FILE    = \"wikipedia.talkpages.conversations.txt\"\n",
    "USERS_FILE    = \"wikipedia.talkpages.userinfo.txt\"\n",
    "ADMINS_FILE   = \"wikipedia.ta~lkpages.admins.txt\"\n",
    "POSTS_DF_FILE = \"posts_df.pickle\"\n",
    "USERS_DF_FILE = \"users_df.pickle\"\n",
    "POSTS_CSV     = \"posts.csv\"\n",
    "USERS_CSV     = \"users.csv\"\n",
    "NETWORK_FILE  = \"users_network.pickle\"\n",
    "CORPUS_DIR    = (\"../data/wiki/\")\n",
    "FWORDS_DIR    = '../data/function words/'\n",
    "\n",
    "function_words = ['conjunctions', 'articles', 'prepositions', 'adverbs', 'quantifiers', \n",
    "           'impersonal_pronouns', 'personal_pronouns', 'auxiliary_verbs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the `posts` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['utterance_id', 'user', 'talkpage_user', 'conversation_root', 'reply_to', \n",
    "           'timestamp', 'timestamp_unixtime', 'clean_text', 'raw_text']\n",
    "\n",
    "posts = {column: [] for column in columns}\n",
    "with open(CORPUS_DIR + CONVO_FILE) as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        \n",
    "        # parse lines from the conversations file\n",
    "        if line.startswith(\"could not match\") or line.strip() == \"\":\n",
    "            continue\n",
    "        line = line.rstrip('\\n').split(DELIM)\n",
    "        assert(len(line) == len(columns))\n",
    "        line = {column: value for column, value in zip(columns, line)}\n",
    "        \n",
    "        # convert timestamps to datetime objects\n",
    "        try:\n",
    "            line['timestamp'] = datetime.strptime(line['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            line['timestamp'] = None\n",
    "            \n",
    "        for column, value in line.items():\n",
    "            posts[column].append(value)\n",
    "\n",
    "            \n",
    "posts = pd.DataFrame(data=posts, index=posts['utterance_id'], columns=columns, dtype=str)\n",
    "\n",
    "# tokenize the post content\n",
    "if not 'tokens' in posts.columns:\n",
    "    tokens = [nltk.tokenize.word_tokenize(text) for text in tqdm(posts['clean_text'])]\n",
    "    posts = posts.assign(tokens=tokens)\n",
    "    \n",
    "# look for markers\n",
    "\n",
    "markers = {feature: [] for feature in function_words}\n",
    "for feature in function_words:\n",
    "    with open(FWORDS_DIR + feature + '.txt') as f:\n",
    "        markers[feature] = [word.rstrip('\\n') for word in f.readlines()]\n",
    "        \n",
    "feature_columns = {m: [False] * len(posts) for m in function_words}       \n",
    "for i, tokens in enumerate(tqdm(posts['tokens'])):\n",
    "    for m in function_words:\n",
    "        if any(t.lower() in markers[m] for t in tokens):\n",
    "            feature_columns[m][i] = True\n",
    "        \n",
    "posts = posts.assign(**feature_columns)\n",
    "\n",
    "# save the dataframe\n",
    "\n",
    "pd.to_pickle(posts, CORPUS_DIR + POSTS_DF_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... or load already-saved posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_pickle(CORPUS_DIR + POSTS_DF_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge posts into reply pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.merge(posts, posts, how='inner', left_index=True, right_on='reply_to', suffixes=['_a', '_b'])\n",
    "\n",
    "# filter out empty users & self-replies\n",
    "pairs = pairs[(pairs.user_a != pairs.user_b) & pairs.user_a & pairs.user_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the input data for Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the marker usage columns for the reply pair\n",
    "for m in function_words:\n",
    "    pairs[m] = list(zip(pairs[m+'_a'], pairs[m+'_b']))\n",
    "\n",
    "# reshape\n",
    "df = pd.melt(pairs, id_vars = ['user_a', 'user_b', 'utterance_id_b'], value_vars=function_words, var_name='marker')\n",
    "\n",
    "# change the marker labels to indices Stan will like\n",
    "marker_idx = {m:i+1 for i,m in enumerate(function_words)}\n",
    "df['marker'] = df['marker'].apply(lambda x: marker_idx[x])\n",
    "\n",
    "# reshape again\n",
    "df = df.pivot_table(index=['user_a', 'user_b', 'marker'], columns='value', aggfunc='size', fill_value=0)\n",
    "df = df.reset_index()\n",
    "\n",
    "# df = df.sample(50000)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    \"NumMarkers\": len(function_words),\n",
    "    \"NumObservations\": len(df),\n",
    "    \"MarkerType\": df.marker.values,\n",
    "    \"NumUtterancesAB\": (df[(True, True)] + df[(True, False)]).values,\n",
    "    \"NumUtterancesNotAB\": (df[(False, True)] + df[(False, False)]).values,\n",
    "    \"CountsAB\": df[(True, True)].values,\n",
    "    \"CountsNotAB\": df[(False, True)].values,\n",
    "    \"StdDev\": .25\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Stan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm = pystan.StanModel(file='alignment.cauchy.nosubpop.stan', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Stan model to the data\n",
    "save the paramteer `eta_ab_pop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "fit = sm.sampling(data=data, iter=200, pars=['eta_ab_pop'], chains=4)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fit.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasn-env",
   "language": "python",
   "name": "lasn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
