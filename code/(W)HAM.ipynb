{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "DELIM         = \" +++$+++ \"\n",
    "CONVO_FILE    = \"wikipedia.talkpages.conversations.txt\"\n",
    "USERS_FILE    = \"wikipedia.talkpages.userinfo.txt\"\n",
    "ADMINS_FILE   = \"wikipedia.ta~lkpages.admins.txt\"\n",
    "POSTS_DF_FILE = \"posts_df.pickle\"\n",
    "USERS_DF_FILE = \"users_df.pickle\"\n",
    "POSTS_CSV     = \"posts.csv\"\n",
    "USERS_CSV     = \"users.csv\"\n",
    "NETWORK_FILE  = \"users_network.pickle\"\n",
    "CORPUS_DIR    = (\"../data/wiki/\")\n",
    "FWORDS_DIR    = '../data/function words/'\n",
    "\n",
    "function_words = ['conjunctions', 'articles', 'prepositions', 'adverbs', 'quantifiers', \n",
    "           'impersonal_pronouns', 'personal_pronouns', 'auxiliary_verbs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the `posts` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['utterance_id', 'user', 'talkpage_user', 'conversation_root', 'reply_to', \n",
    "           'timestamp', 'timestamp_unixtime', 'clean_text', 'raw_text']\n",
    "\n",
    "posts = {column: [] for column in columns}\n",
    "with open(CORPUS_DIR + CONVO_FILE) as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        \n",
    "        # parse lines from the conversations file\n",
    "        if line.startswith(\"could not match\") or line.strip() == \"\":\n",
    "            continue\n",
    "        line = line.rstrip('\\n').split(DELIM)\n",
    "        assert(len(line) == len(columns))\n",
    "        line = {column: value for column, value in zip(columns, line)}\n",
    "        \n",
    "        # convert timestamps to datetime objects\n",
    "        try:\n",
    "            line['timestamp'] = datetime.strptime(line['timestamp'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            line['timestamp'] = None\n",
    "            \n",
    "        for column, value in line.items():\n",
    "            posts[column].append(value)\n",
    "\n",
    "            \n",
    "posts = pd.DataFrame(data=posts, index=posts['utterance_id'], columns=columns, dtype=str)\n",
    "\n",
    "# tokenize the post content\n",
    "if not 'tokens' in posts.columns:\n",
    "    tokens = [nltk.tokenize.word_tokenize(text) for text in tqdm(posts['clean_text'])]\n",
    "    posts = posts.assign(tokens=tokens)\n",
    "    \n",
    "# look for markers\n",
    "\n",
    "markers = {feature: [] for feature in function_words}\n",
    "for feature in function_words:\n",
    "    with open(FWORDS_DIR + feature + '.txt') as f:\n",
    "        markers[feature] = [word.rstrip('\\n') for word in f.readlines()]\n",
    "        \n",
    "feature_columns = {m: [False] * len(posts) for m in function_words}       \n",
    "for i, tokens in enumerate(tqdm(posts['tokens'])):\n",
    "    for m in function_words:\n",
    "        if any(t.lower() in markers[m] for t in tokens):\n",
    "            feature_columns[m][i] = True\n",
    "        \n",
    "posts = posts.assign(**feature_columns)\n",
    "\n",
    "# save the dataframe\n",
    "\n",
    "pd.to_pickle(posts, CORPUS_DIR + POSTS_DF_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... or load already-saved posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_pickle(CORPUS_DIR + POSTS_DF_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge posts into reply pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.merge(posts, posts, how='inner', left_index=True, right_on='reply_to', suffixes=['_a', '_b'])\n",
    "# TODO: filter out empty users & self-replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in function_words:\n",
    "    pairs[m] = list(zip(pairs[m+'_a'], pairs[m+'_b']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.set_index(['utterance_id_b', 'user_a', 'user_b'])[function_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pairs.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pairs.groupby(['user_a', 'user_b'] + marker_usage).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_markers = len(function_words)\n",
    "num_observations = len(df)\n",
    "marker_type = np.array([1] * num_observations)\n",
    "num_utterances_ab = (df[(True, True)] + df[(True, False)]).values\n",
    "num_utterances_not_ab = (df[(False, True)] + df[(False, False)]).values\n",
    "counts_ab = df[(True, True)].values\n",
    "counts_not_ab = df[(False, True)].values\n",
    "stddev = .25\n",
    "\n",
    "data = {\n",
    "    \"NumMarkers\": num_markers,\n",
    "    \"NumObservations\": num_observations,\n",
    "    \"MarkerType\": marker_type,\n",
    "    \"NumUtterancesAB\": num_utterances_ab,\n",
    "    \"NumUtterancesNotAB\": num_utterances_not_ab,\n",
    "    \"CountsAB\": counts_ab,\n",
    "    \"CountsNotAB\": counts_not_ab,\n",
    "    \"StdDev\": stddev\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "sm = pystan.StanModel(file='../../disc_align/models/alignment.cauchy.nosubpop.stan', verbose=True)\n",
    "fit = sm.sampling(data=data, iter=200, chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fit.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasn-env",
   "language": "python",
   "name": "lasn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
