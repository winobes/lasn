{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4abc4ca0b56402bb8092c3242302a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    29007\n",
       "True      1893\n",
       "Name: highly_central, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import wiki\n",
    "from data import corpus\n",
    "import alignment\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "tqdm().pandas()\n",
    "\n",
    "posts = wiki.load_posts()\n",
    "pairs = corpus.get_reply_pairs(posts)\n",
    "users = wiki.load_users(posts=posts)\n",
    "network = wiki.load_network(reply_pairs=pairs, recreate=False)\n",
    "\n",
    "df = pd.merge(posts, users, left_on='user', right_index=True)\n",
    "\n",
    "threshold = users.centrality.mean() + users.centrality.std()\n",
    "users['highly_central'] = (users['centrality'] > threshold)\n",
    "df['highly_central'] = (df['centrality'] > threshold)\n",
    "users.highly_central.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = posts.tokens.apply(nltk.pos_tag)\n",
    "pos_tags_only = pos_tags.apply(lambda x: [y[1] for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, series, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "        tokens = 0\n",
    "        for line in series:\n",
    "            words = line + ['<eos>']\n",
    "            tokens += len(words)\n",
    "            for word in words: \n",
    "                self.dictionary.add_word(word)  \n",
    "\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        for line in series:\n",
    "            words = line + ['<eos>']\n",
    "            for word in words:\n",
    "                ids[token] = self.dictionary.word2idx[word]\n",
    "                token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "hidden_size = 200\n",
    "embedding_size = 50\n",
    "seq_length = 30\n",
    "num_epochs = 1\n",
    "model = RNNLM(len(corpus.dictionary), embedding_size, hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "data = corpus.get_data(pos_tags_only, batch_size)\n",
    "\n",
    "num_batches = data.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step[0/38059], Loss: 3.0300, Perplexity: 20.70\n",
      "Epoch [1/1], Step[100/38059], Loss: 2.3599, Perplexity: 10.59\n",
      "Epoch [1/1], Step[200/38059], Loss: 2.2038, Perplexity:  9.06\n",
      "Epoch [1/1], Step[300/38059], Loss: 2.4060, Perplexity: 11.09\n",
      "Epoch [1/1], Step[400/38059], Loss: 2.3193, Perplexity: 10.17\n",
      "Epoch [1/1], Step[500/38059], Loss: 2.3512, Perplexity: 10.50\n",
      "Epoch [1/1], Step[600/38059], Loss: 2.1364, Perplexity:  8.47\n",
      "Epoch [1/1], Step[700/38059], Loss: 2.1374, Perplexity:  8.48\n",
      "Epoch [1/1], Step[800/38059], Loss: 2.2399, Perplexity:  9.39\n",
      "Epoch [1/1], Step[900/38059], Loss: 2.2031, Perplexity:  9.05\n",
      "Epoch [1/1], Step[1000/38059], Loss: 2.2540, Perplexity:  9.53\n",
      "Epoch [1/1], Step[1100/38059], Loss: 2.1650, Perplexity:  8.71\n",
      "Epoch [1/1], Step[1200/38059], Loss: 2.2341, Perplexity:  9.34\n",
      "Epoch [1/1], Step[1300/38059], Loss: 2.1859, Perplexity:  8.90\n",
      "Epoch [1/1], Step[1400/38059], Loss: 2.1410, Perplexity:  8.51\n",
      "Epoch [1/1], Step[1500/38059], Loss: 2.2121, Perplexity:  9.14\n",
      "Epoch [1/1], Step[1600/38059], Loss: 2.1988, Perplexity:  9.01\n",
      "Epoch [1/1], Step[1700/38059], Loss: 2.3150, Perplexity: 10.13\n",
      "Epoch [1/1], Step[1800/38059], Loss: 2.1528, Perplexity:  8.61\n",
      "Epoch [1/1], Step[1900/38059], Loss: 2.2213, Perplexity:  9.22\n",
      "Epoch [1/1], Step[2000/38059], Loss: 2.1312, Perplexity:  8.42\n",
      "Epoch [1/1], Step[2100/38059], Loss: 2.2640, Perplexity:  9.62\n",
      "Epoch [1/1], Step[2200/38059], Loss: 1.9600, Perplexity:  7.10\n",
      "Epoch [1/1], Step[2300/38059], Loss: 2.1086, Perplexity:  8.24\n",
      "Epoch [1/1], Step[2400/38059], Loss: 2.1312, Perplexity:  8.42\n",
      "Epoch [1/1], Step[2500/38059], Loss: 2.1587, Perplexity:  8.66\n",
      "Epoch [1/1], Step[2600/38059], Loss: 2.1190, Perplexity:  8.32\n",
      "Epoch [1/1], Step[2700/38059], Loss: 2.2435, Perplexity:  9.43\n",
      "Epoch [1/1], Step[2800/38059], Loss: 2.1385, Perplexity:  8.49\n",
      "Epoch [1/1], Step[2900/38059], Loss: 2.1189, Perplexity:  8.32\n",
      "Epoch [1/1], Step[3000/38059], Loss: 2.2438, Perplexity:  9.43\n",
      "Epoch [1/1], Step[3100/38059], Loss: 2.0857, Perplexity:  8.05\n",
      "Epoch [1/1], Step[3200/38059], Loss: 2.1166, Perplexity:  8.30\n",
      "Epoch [1/1], Step[3300/38059], Loss: 2.1957, Perplexity:  8.99\n",
      "Epoch [1/1], Step[3400/38059], Loss: 2.0339, Perplexity:  7.64\n",
      "Epoch [1/1], Step[3500/38059], Loss: 2.1051, Perplexity:  8.21\n",
      "Epoch [1/1], Step[3600/38059], Loss: 2.1627, Perplexity:  8.69\n",
      "Epoch [1/1], Step[3700/38059], Loss: 2.1341, Perplexity:  8.45\n",
      "Epoch [1/1], Step[3800/38059], Loss: 2.2486, Perplexity:  9.47\n",
      "Epoch [1/1], Step[3900/38059], Loss: 2.0571, Perplexity:  7.82\n",
      "Epoch [1/1], Step[4000/38059], Loss: 2.1321, Perplexity:  8.43\n",
      "Epoch [1/1], Step[4100/38059], Loss: 2.2739, Perplexity:  9.72\n",
      "Epoch [1/1], Step[4200/38059], Loss: 2.1025, Perplexity:  8.19\n",
      "Epoch [1/1], Step[4300/38059], Loss: 2.0787, Perplexity:  7.99\n",
      "Epoch [1/1], Step[4400/38059], Loss: 2.1638, Perplexity:  8.70\n",
      "Epoch [1/1], Step[4500/38059], Loss: 2.0612, Perplexity:  7.86\n",
      "Epoch [1/1], Step[4600/38059], Loss: 2.0465, Perplexity:  7.74\n",
      "Epoch [1/1], Step[4700/38059], Loss: 2.0641, Perplexity:  7.88\n",
      "Epoch [1/1], Step[4800/38059], Loss: 2.1334, Perplexity:  8.44\n",
      "Epoch [1/1], Step[4900/38059], Loss: 2.1227, Perplexity:  8.35\n",
      "Epoch [1/1], Step[5000/38059], Loss: 2.1198, Perplexity:  8.33\n",
      "Epoch [1/1], Step[5100/38059], Loss: 2.2015, Perplexity:  9.04\n",
      "Epoch [1/1], Step[5200/38059], Loss: 2.1420, Perplexity:  8.52\n",
      "Epoch [1/1], Step[5300/38059], Loss: 2.1462, Perplexity:  8.55\n",
      "Epoch [1/1], Step[5400/38059], Loss: 2.2603, Perplexity:  9.59\n",
      "Epoch [1/1], Step[5500/38059], Loss: 2.1606, Perplexity:  8.68\n",
      "Epoch [1/1], Step[5600/38059], Loss: 2.1147, Perplexity:  8.29\n",
      "Epoch [1/1], Step[5700/38059], Loss: 2.1211, Perplexity:  8.34\n",
      "Epoch [1/1], Step[5800/38059], Loss: 2.0979, Perplexity:  8.15\n",
      "Epoch [1/1], Step[5900/38059], Loss: 2.1620, Perplexity:  8.69\n",
      "Epoch [1/1], Step[6000/38059], Loss: 2.1082, Perplexity:  8.23\n",
      "Epoch [1/1], Step[6100/38059], Loss: 2.0385, Perplexity:  7.68\n",
      "Epoch [1/1], Step[6200/38059], Loss: 2.1841, Perplexity:  8.88\n",
      "Epoch [1/1], Step[6300/38059], Loss: 2.2291, Perplexity:  9.29\n",
      "Epoch [1/1], Step[6400/38059], Loss: 2.2358, Perplexity:  9.35\n",
      "Epoch [1/1], Step[6500/38059], Loss: 2.1362, Perplexity:  8.47\n",
      "Epoch [1/1], Step[6600/38059], Loss: 2.1877, Perplexity:  8.91\n",
      "Epoch [1/1], Step[6700/38059], Loss: 2.0730, Perplexity:  7.95\n",
      "Epoch [1/1], Step[6800/38059], Loss: 2.1809, Perplexity:  8.85\n",
      "Epoch [1/1], Step[6900/38059], Loss: 2.1458, Perplexity:  8.55\n",
      "Epoch [1/1], Step[7000/38059], Loss: 2.1929, Perplexity:  8.96\n",
      "Epoch [1/1], Step[7100/38059], Loss: 2.1872, Perplexity:  8.91\n",
      "Epoch [1/1], Step[7200/38059], Loss: 2.1744, Perplexity:  8.80\n",
      "Epoch [1/1], Step[7300/38059], Loss: 2.0198, Perplexity:  7.54\n",
      "Epoch [1/1], Step[7400/38059], Loss: 2.1404, Perplexity:  8.50\n",
      "Epoch [1/1], Step[7500/38059], Loss: 2.2373, Perplexity:  9.37\n",
      "Epoch [1/1], Step[7600/38059], Loss: 2.1284, Perplexity:  8.40\n",
      "Epoch [1/1], Step[7700/38059], Loss: 2.0734, Perplexity:  7.95\n",
      "Epoch [1/1], Step[7800/38059], Loss: 2.1937, Perplexity:  8.97\n",
      "Epoch [1/1], Step[7900/38059], Loss: 2.0919, Perplexity:  8.10\n",
      "Epoch [1/1], Step[8000/38059], Loss: 2.1608, Perplexity:  8.68\n",
      "Epoch [1/1], Step[8100/38059], Loss: 1.9869, Perplexity:  7.29\n",
      "Epoch [1/1], Step[8200/38059], Loss: 2.0272, Perplexity:  7.59\n",
      "Epoch [1/1], Step[8300/38059], Loss: 2.1978, Perplexity:  9.01\n",
      "Epoch [1/1], Step[8400/38059], Loss: 2.0188, Perplexity:  7.53\n",
      "Epoch [1/1], Step[8500/38059], Loss: 2.0050, Perplexity:  7.43\n",
      "Epoch [1/1], Step[8600/38059], Loss: 2.2031, Perplexity:  9.05\n",
      "Epoch [1/1], Step[8700/38059], Loss: 2.1355, Perplexity:  8.46\n",
      "Epoch [1/1], Step[8800/38059], Loss: 2.1792, Perplexity:  8.84\n",
      "Epoch [1/1], Step[8900/38059], Loss: 2.0262, Perplexity:  7.58\n",
      "Epoch [1/1], Step[9000/38059], Loss: 2.2371, Perplexity:  9.37\n",
      "Epoch [1/1], Step[9100/38059], Loss: 2.1451, Perplexity:  8.54\n",
      "Epoch [1/1], Step[9200/38059], Loss: 2.1645, Perplexity:  8.71\n",
      "Epoch [1/1], Step[9300/38059], Loss: 2.0217, Perplexity:  7.55\n",
      "Epoch [1/1], Step[9400/38059], Loss: 2.0792, Perplexity:  8.00\n",
      "Epoch [1/1], Step[9500/38059], Loss: 2.1355, Perplexity:  8.46\n",
      "Epoch [1/1], Step[9600/38059], Loss: 2.2525, Perplexity:  9.51\n",
      "Epoch [1/1], Step[9700/38059], Loss: 2.1478, Perplexity:  8.57\n",
      "Epoch [1/1], Step[9800/38059], Loss: 2.2439, Perplexity:  9.43\n",
      "Epoch [1/1], Step[9900/38059], Loss: 2.2208, Perplexity:  9.21\n",
      "Epoch [1/1], Step[10000/38059], Loss: 2.2175, Perplexity:  9.18\n",
      "Epoch [1/1], Step[10100/38059], Loss: 2.0033, Perplexity:  7.41\n",
      "Epoch [1/1], Step[10200/38059], Loss: 2.2380, Perplexity:  9.37\n",
      "Epoch [1/1], Step[10300/38059], Loss: 2.0503, Perplexity:  7.77\n",
      "Epoch [1/1], Step[10400/38059], Loss: 2.0595, Perplexity:  7.84\n",
      "Epoch [1/1], Step[10500/38059], Loss: 2.1950, Perplexity:  8.98\n",
      "Epoch [1/1], Step[10600/38059], Loss: 2.2087, Perplexity:  9.10\n",
      "Epoch [1/1], Step[10700/38059], Loss: 2.1149, Perplexity:  8.29\n",
      "Epoch [1/1], Step[10800/38059], Loss: 2.1937, Perplexity:  8.97\n",
      "Epoch [1/1], Step[10900/38059], Loss: 2.0704, Perplexity:  7.93\n",
      "Epoch [1/1], Step[11000/38059], Loss: 2.1141, Perplexity:  8.28\n",
      "Epoch [1/1], Step[11100/38059], Loss: 2.0869, Perplexity:  8.06\n",
      "Epoch [1/1], Step[11200/38059], Loss: 2.2580, Perplexity:  9.56\n",
      "Epoch [1/1], Step[11300/38059], Loss: 2.2365, Perplexity:  9.36\n",
      "Epoch [1/1], Step[11400/38059], Loss: 2.1116, Perplexity:  8.26\n",
      "Epoch [1/1], Step[11500/38059], Loss: 2.1434, Perplexity:  8.53\n",
      "Epoch [1/1], Step[11600/38059], Loss: 2.0971, Perplexity:  8.14\n",
      "Epoch [1/1], Step[11700/38059], Loss: 2.0736, Perplexity:  7.95\n",
      "Epoch [1/1], Step[11800/38059], Loss: 2.1857, Perplexity:  8.90\n",
      "Epoch [1/1], Step[11900/38059], Loss: 2.2047, Perplexity:  9.07\n",
      "Epoch [1/1], Step[12000/38059], Loss: 2.0978, Perplexity:  8.15\n",
      "Epoch [1/1], Step[12100/38059], Loss: 2.2221, Perplexity:  9.23\n",
      "Epoch [1/1], Step[12200/38059], Loss: 2.1583, Perplexity:  8.66\n",
      "Epoch [1/1], Step[12300/38059], Loss: 2.1364, Perplexity:  8.47\n",
      "Epoch [1/1], Step[12400/38059], Loss: 2.2457, Perplexity:  9.45\n",
      "Epoch [1/1], Step[12500/38059], Loss: 2.1707, Perplexity:  8.76\n",
      "Epoch [1/1], Step[12600/38059], Loss: 2.1515, Perplexity:  8.60\n",
      "Epoch [1/1], Step[12700/38059], Loss: 2.1516, Perplexity:  8.60\n",
      "Epoch [1/1], Step[12800/38059], Loss: 2.0168, Perplexity:  7.51\n",
      "Epoch [1/1], Step[12900/38059], Loss: 2.0044, Perplexity:  7.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step[13000/38059], Loss: 2.1132, Perplexity:  8.27\n",
      "Epoch [1/1], Step[13100/38059], Loss: 2.1356, Perplexity:  8.46\n",
      "Epoch [1/1], Step[13200/38059], Loss: 2.1821, Perplexity:  8.87\n",
      "Epoch [1/1], Step[13300/38059], Loss: 2.1223, Perplexity:  8.35\n",
      "Epoch [1/1], Step[13400/38059], Loss: 2.0002, Perplexity:  7.39\n",
      "Epoch [1/1], Step[13500/38059], Loss: 2.1746, Perplexity:  8.80\n",
      "Epoch [1/1], Step[13600/38059], Loss: 1.9798, Perplexity:  7.24\n",
      "Epoch [1/1], Step[13700/38059], Loss: 2.2077, Perplexity:  9.10\n",
      "Epoch [1/1], Step[13800/38059], Loss: 2.0405, Perplexity:  7.69\n",
      "Epoch [1/1], Step[13900/38059], Loss: 2.1770, Perplexity:  8.82\n",
      "Epoch [1/1], Step[14000/38059], Loss: 2.1220, Perplexity:  8.35\n",
      "Epoch [1/1], Step[14100/38059], Loss: 2.1523, Perplexity:  8.60\n",
      "Epoch [1/1], Step[14200/38059], Loss: 2.1695, Perplexity:  8.75\n",
      "Epoch [1/1], Step[14300/38059], Loss: 2.2626, Perplexity:  9.61\n",
      "Epoch [1/1], Step[14400/38059], Loss: 2.1097, Perplexity:  8.25\n",
      "Epoch [1/1], Step[14500/38059], Loss: 2.0329, Perplexity:  7.64\n",
      "Epoch [1/1], Step[14600/38059], Loss: 2.1409, Perplexity:  8.51\n",
      "Epoch [1/1], Step[14700/38059], Loss: 2.1719, Perplexity:  8.77\n",
      "Epoch [1/1], Step[14800/38059], Loss: 2.1320, Perplexity:  8.43\n",
      "Epoch [1/1], Step[14900/38059], Loss: 2.0572, Perplexity:  7.82\n",
      "Epoch [1/1], Step[15000/38059], Loss: 2.2613, Perplexity:  9.60\n",
      "Epoch [1/1], Step[15100/38059], Loss: 2.2604, Perplexity:  9.59\n",
      "Epoch [1/1], Step[15200/38059], Loss: 2.0883, Perplexity:  8.07\n",
      "Epoch [1/1], Step[15300/38059], Loss: 2.1985, Perplexity:  9.01\n",
      "Epoch [1/1], Step[15400/38059], Loss: 2.0344, Perplexity:  7.65\n",
      "Epoch [1/1], Step[15500/38059], Loss: 2.1487, Perplexity:  8.57\n",
      "Epoch [1/1], Step[15600/38059], Loss: 2.1967, Perplexity:  8.99\n",
      "Epoch [1/1], Step[15700/38059], Loss: 2.0499, Perplexity:  7.77\n",
      "Epoch [1/1], Step[15800/38059], Loss: 2.0144, Perplexity:  7.50\n",
      "Epoch [1/1], Step[15900/38059], Loss: 2.0782, Perplexity:  7.99\n",
      "Epoch [1/1], Step[16000/38059], Loss: 2.1140, Perplexity:  8.28\n",
      "Epoch [1/1], Step[16100/38059], Loss: 2.1152, Perplexity:  8.29\n",
      "Epoch [1/1], Step[16200/38059], Loss: 2.2121, Perplexity:  9.13\n",
      "Epoch [1/1], Step[16300/38059], Loss: 2.2196, Perplexity:  9.20\n",
      "Epoch [1/1], Step[16400/38059], Loss: 2.1118, Perplexity:  8.26\n",
      "Epoch [1/1], Step[16500/38059], Loss: 2.1417, Perplexity:  8.51\n",
      "Epoch [1/1], Step[16600/38059], Loss: 2.1298, Perplexity:  8.41\n",
      "Epoch [1/1], Step[16700/38059], Loss: 2.0787, Perplexity:  7.99\n",
      "Epoch [1/1], Step[16800/38059], Loss: 2.2433, Perplexity:  9.42\n",
      "Epoch [1/1], Step[16900/38059], Loss: 2.1231, Perplexity:  8.36\n",
      "Epoch [1/1], Step[17000/38059], Loss: 2.2812, Perplexity:  9.79\n",
      "Epoch [1/1], Step[17100/38059], Loss: 2.1555, Perplexity:  8.63\n",
      "Epoch [1/1], Step[17200/38059], Loss: 2.0344, Perplexity:  7.65\n",
      "Epoch [1/1], Step[17300/38059], Loss: 2.1426, Perplexity:  8.52\n",
      "Epoch [1/1], Step[17400/38059], Loss: 2.0816, Perplexity:  8.02\n",
      "Epoch [1/1], Step[17500/38059], Loss: 2.1656, Perplexity:  8.72\n",
      "Epoch [1/1], Step[17600/38059], Loss: 2.0683, Perplexity:  7.91\n",
      "Epoch [1/1], Step[17700/38059], Loss: 2.0600, Perplexity:  7.85\n",
      "Epoch [1/1], Step[17800/38059], Loss: 2.1303, Perplexity:  8.42\n",
      "Epoch [1/1], Step[17900/38059], Loss: 2.0771, Perplexity:  7.98\n",
      "Epoch [1/1], Step[18000/38059], Loss: 2.0518, Perplexity:  7.78\n",
      "Epoch [1/1], Step[18100/38059], Loss: 2.0938, Perplexity:  8.12\n",
      "Epoch [1/1], Step[18200/38059], Loss: 2.1941, Perplexity:  8.97\n",
      "Epoch [1/1], Step[18300/38059], Loss: 2.0351, Perplexity:  7.65\n",
      "Epoch [1/1], Step[18400/38059], Loss: 2.0423, Perplexity:  7.71\n",
      "Epoch [1/1], Step[18500/38059], Loss: 2.2737, Perplexity:  9.72\n",
      "Epoch [1/1], Step[18600/38059], Loss: 2.0663, Perplexity:  7.90\n",
      "Epoch [1/1], Step[18700/38059], Loss: 2.1748, Perplexity:  8.80\n",
      "Epoch [1/1], Step[18800/38059], Loss: 2.1338, Perplexity:  8.45\n",
      "Epoch [1/1], Step[18900/38059], Loss: 2.1299, Perplexity:  8.41\n",
      "Epoch [1/1], Step[19000/38059], Loss: 2.1354, Perplexity:  8.46\n",
      "Epoch [1/1], Step[19100/38059], Loss: 2.1954, Perplexity:  8.98\n",
      "Epoch [1/1], Step[19200/38059], Loss: 2.0988, Perplexity:  8.16\n",
      "Epoch [1/1], Step[19300/38059], Loss: 2.1197, Perplexity:  8.33\n",
      "Epoch [1/1], Step[19400/38059], Loss: 2.0416, Perplexity:  7.70\n",
      "Epoch [1/1], Step[19500/38059], Loss: 2.0916, Perplexity:  8.10\n",
      "Epoch [1/1], Step[19600/38059], Loss: 2.2478, Perplexity:  9.47\n",
      "Epoch [1/1], Step[19700/38059], Loss: 2.1102, Perplexity:  8.25\n",
      "Epoch [1/1], Step[19800/38059], Loss: 2.2401, Perplexity:  9.39\n",
      "Epoch [1/1], Step[19900/38059], Loss: 2.2091, Perplexity:  9.11\n",
      "Epoch [1/1], Step[20000/38059], Loss: 1.9868, Perplexity:  7.29\n",
      "Epoch [1/1], Step[20100/38059], Loss: 2.0310, Perplexity:  7.62\n",
      "Epoch [1/1], Step[20200/38059], Loss: 2.1839, Perplexity:  8.88\n",
      "Epoch [1/1], Step[20300/38059], Loss: 2.1479, Perplexity:  8.57\n",
      "Epoch [1/1], Step[20400/38059], Loss: 2.1647, Perplexity:  8.71\n",
      "Epoch [1/1], Step[20500/38059], Loss: 2.0966, Perplexity:  8.14\n",
      "Epoch [1/1], Step[20600/38059], Loss: 2.1225, Perplexity:  8.35\n",
      "Epoch [1/1], Step[20700/38059], Loss: 2.3165, Perplexity: 10.14\n",
      "Epoch [1/1], Step[20800/38059], Loss: 2.2128, Perplexity:  9.14\n",
      "Epoch [1/1], Step[20900/38059], Loss: 2.0594, Perplexity:  7.84\n",
      "Epoch [1/1], Step[21000/38059], Loss: 2.2226, Perplexity:  9.23\n",
      "Epoch [1/1], Step[21100/38059], Loss: 2.2195, Perplexity:  9.20\n",
      "Epoch [1/1], Step[21200/38059], Loss: 2.2352, Perplexity:  9.35\n",
      "Epoch [1/1], Step[21300/38059], Loss: 2.2449, Perplexity:  9.44\n",
      "Epoch [1/1], Step[21400/38059], Loss: 2.0863, Perplexity:  8.05\n",
      "Epoch [1/1], Step[21500/38059], Loss: 2.2866, Perplexity:  9.84\n",
      "Epoch [1/1], Step[21600/38059], Loss: 2.1578, Perplexity:  8.65\n",
      "Epoch [1/1], Step[21700/38059], Loss: 2.0807, Perplexity:  8.01\n",
      "Epoch [1/1], Step[21800/38059], Loss: 2.1664, Perplexity:  8.73\n",
      "Epoch [1/1], Step[21900/38059], Loss: 2.1603, Perplexity:  8.67\n",
      "Epoch [1/1], Step[22000/38059], Loss: 2.0478, Perplexity:  7.75\n",
      "Epoch [1/1], Step[22100/38059], Loss: 2.2609, Perplexity:  9.59\n",
      "Epoch [1/1], Step[22200/38059], Loss: 2.1511, Perplexity:  8.59\n",
      "Epoch [1/1], Step[22300/38059], Loss: 2.0116, Perplexity:  7.48\n",
      "Epoch [1/1], Step[22400/38059], Loss: 2.0687, Perplexity:  7.91\n",
      "Epoch [1/1], Step[22500/38059], Loss: 2.1909, Perplexity:  8.94\n",
      "Epoch [1/1], Step[22600/38059], Loss: 2.2388, Perplexity:  9.38\n",
      "Epoch [1/1], Step[22700/38059], Loss: 2.2110, Perplexity:  9.12\n",
      "Epoch [1/1], Step[22800/38059], Loss: 2.1344, Perplexity:  8.45\n",
      "Epoch [1/1], Step[22900/38059], Loss: 2.1371, Perplexity:  8.47\n",
      "Epoch [1/1], Step[23000/38059], Loss: 2.1783, Perplexity:  8.83\n",
      "Epoch [1/1], Step[23100/38059], Loss: 2.2326, Perplexity:  9.32\n",
      "Epoch [1/1], Step[23200/38059], Loss: 1.9860, Perplexity:  7.29\n",
      "Epoch [1/1], Step[23300/38059], Loss: 1.9787, Perplexity:  7.23\n",
      "Epoch [1/1], Step[23400/38059], Loss: 1.9550, Perplexity:  7.06\n",
      "Epoch [1/1], Step[23500/38059], Loss: 2.1784, Perplexity:  8.83\n",
      "Epoch [1/1], Step[23600/38059], Loss: 2.0989, Perplexity:  8.16\n",
      "Epoch [1/1], Step[23700/38059], Loss: 2.1490, Perplexity:  8.58\n",
      "Epoch [1/1], Step[23800/38059], Loss: 2.0569, Perplexity:  7.82\n",
      "Epoch [1/1], Step[23900/38059], Loss: 2.0640, Perplexity:  7.88\n",
      "Epoch [1/1], Step[24000/38059], Loss: 2.0681, Perplexity:  7.91\n",
      "Epoch [1/1], Step[24100/38059], Loss: 2.0462, Perplexity:  7.74\n",
      "Epoch [1/1], Step[24200/38059], Loss: 2.1973, Perplexity:  9.00\n",
      "Epoch [1/1], Step[24300/38059], Loss: 2.1213, Perplexity:  8.34\n",
      "Epoch [1/1], Step[24400/38059], Loss: 2.1318, Perplexity:  8.43\n",
      "Epoch [1/1], Step[24500/38059], Loss: 2.0941, Perplexity:  8.12\n",
      "Epoch [1/1], Step[24600/38059], Loss: 2.0873, Perplexity:  8.06\n",
      "Epoch [1/1], Step[24700/38059], Loss: 2.1563, Perplexity:  8.64\n",
      "Epoch [1/1], Step[24800/38059], Loss: 2.1872, Perplexity:  8.91\n",
      "Epoch [1/1], Step[24900/38059], Loss: 2.0912, Perplexity:  8.09\n",
      "Epoch [1/1], Step[25000/38059], Loss: 2.1032, Perplexity:  8.19\n",
      "Epoch [1/1], Step[25100/38059], Loss: 2.0154, Perplexity:  7.50\n",
      "Epoch [1/1], Step[25200/38059], Loss: 2.1479, Perplexity:  8.57\n",
      "Epoch [1/1], Step[25300/38059], Loss: 2.0580, Perplexity:  7.83\n",
      "Epoch [1/1], Step[25400/38059], Loss: 2.0944, Perplexity:  8.12\n",
      "Epoch [1/1], Step[25500/38059], Loss: 2.0621, Perplexity:  7.86\n",
      "Epoch [1/1], Step[25600/38059], Loss: 2.1383, Perplexity:  8.49\n",
      "Epoch [1/1], Step[25700/38059], Loss: 2.1850, Perplexity:  8.89\n",
      "Epoch [1/1], Step[25800/38059], Loss: 2.3333, Perplexity: 10.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step[25900/38059], Loss: 2.2116, Perplexity:  9.13\n",
      "Epoch [1/1], Step[26000/38059], Loss: 2.0234, Perplexity:  7.56\n",
      "Epoch [1/1], Step[26100/38059], Loss: 2.0130, Perplexity:  7.49\n",
      "Epoch [1/1], Step[26200/38059], Loss: 2.0342, Perplexity:  7.65\n",
      "Epoch [1/1], Step[26300/38059], Loss: 2.1823, Perplexity:  8.87\n",
      "Epoch [1/1], Step[26400/38059], Loss: 1.9995, Perplexity:  7.39\n",
      "Epoch [1/1], Step[26500/38059], Loss: 2.1187, Perplexity:  8.32\n",
      "Epoch [1/1], Step[26600/38059], Loss: 2.0233, Perplexity:  7.56\n",
      "Epoch [1/1], Step[26700/38059], Loss: 2.0948, Perplexity:  8.12\n",
      "Epoch [1/1], Step[26800/38059], Loss: 2.1766, Perplexity:  8.82\n",
      "Epoch [1/1], Step[26900/38059], Loss: 2.0650, Perplexity:  7.89\n",
      "Epoch [1/1], Step[27000/38059], Loss: 2.0980, Perplexity:  8.15\n",
      "Epoch [1/1], Step[27100/38059], Loss: 2.1229, Perplexity:  8.36\n",
      "Epoch [1/1], Step[27200/38059], Loss: 2.1182, Perplexity:  8.32\n",
      "Epoch [1/1], Step[27300/38059], Loss: 2.1526, Perplexity:  8.61\n",
      "Epoch [1/1], Step[27400/38059], Loss: 2.0911, Perplexity:  8.09\n",
      "Epoch [1/1], Step[27500/38059], Loss: 2.1227, Perplexity:  8.35\n",
      "Epoch [1/1], Step[27600/38059], Loss: 2.1444, Perplexity:  8.54\n",
      "Epoch [1/1], Step[27700/38059], Loss: 2.0693, Perplexity:  7.92\n",
      "Epoch [1/1], Step[27800/38059], Loss: 2.1750, Perplexity:  8.80\n",
      "Epoch [1/1], Step[27900/38059], Loss: 2.0041, Perplexity:  7.42\n",
      "Epoch [1/1], Step[28000/38059], Loss: 2.1467, Perplexity:  8.56\n",
      "Epoch [1/1], Step[28100/38059], Loss: 2.1289, Perplexity:  8.41\n",
      "Epoch [1/1], Step[28200/38059], Loss: 2.0303, Perplexity:  7.62\n",
      "Epoch [1/1], Step[28300/38059], Loss: 2.1694, Perplexity:  8.75\n",
      "Epoch [1/1], Step[28400/38059], Loss: 2.0719, Perplexity:  7.94\n",
      "Epoch [1/1], Step[28500/38059], Loss: 2.2862, Perplexity:  9.84\n",
      "Epoch [1/1], Step[28600/38059], Loss: 2.1008, Perplexity:  8.17\n",
      "Epoch [1/1], Step[28700/38059], Loss: 2.1228, Perplexity:  8.35\n",
      "Epoch [1/1], Step[28800/38059], Loss: 2.1680, Perplexity:  8.74\n",
      "Epoch [1/1], Step[28900/38059], Loss: 2.0996, Perplexity:  8.16\n",
      "Epoch [1/1], Step[29000/38059], Loss: 2.1919, Perplexity:  8.95\n",
      "Epoch [1/1], Step[29100/38059], Loss: 2.0683, Perplexity:  7.91\n",
      "Epoch [1/1], Step[29200/38059], Loss: 2.1620, Perplexity:  8.69\n",
      "Epoch [1/1], Step[29300/38059], Loss: 2.1789, Perplexity:  8.84\n",
      "Epoch [1/1], Step[29400/38059], Loss: 2.1072, Perplexity:  8.22\n",
      "Epoch [1/1], Step[29500/38059], Loss: 2.1475, Perplexity:  8.56\n",
      "Epoch [1/1], Step[29600/38059], Loss: 2.1666, Perplexity:  8.73\n",
      "Epoch [1/1], Step[29700/38059], Loss: 2.1524, Perplexity:  8.61\n",
      "Epoch [1/1], Step[29800/38059], Loss: 2.1587, Perplexity:  8.66\n",
      "Epoch [1/1], Step[29900/38059], Loss: 2.1002, Perplexity:  8.17\n",
      "Epoch [1/1], Step[30000/38059], Loss: 2.2108, Perplexity:  9.12\n",
      "Epoch [1/1], Step[30100/38059], Loss: 2.1360, Perplexity:  8.47\n",
      "Epoch [1/1], Step[30200/38059], Loss: 2.2299, Perplexity:  9.30\n",
      "Epoch [1/1], Step[30300/38059], Loss: 2.1088, Perplexity:  8.24\n",
      "Epoch [1/1], Step[30400/38059], Loss: 2.1900, Perplexity:  8.94\n",
      "Epoch [1/1], Step[30500/38059], Loss: 2.1515, Perplexity:  8.60\n",
      "Epoch [1/1], Step[30600/38059], Loss: 2.0328, Perplexity:  7.64\n",
      "Epoch [1/1], Step[30700/38059], Loss: 2.1474, Perplexity:  8.56\n",
      "Epoch [1/1], Step[30800/38059], Loss: 2.1485, Perplexity:  8.57\n",
      "Epoch [1/1], Step[30900/38059], Loss: 2.1152, Perplexity:  8.29\n",
      "Epoch [1/1], Step[31000/38059], Loss: 1.9128, Perplexity:  6.77\n",
      "Epoch [1/1], Step[31100/38059], Loss: 2.0909, Perplexity:  8.09\n",
      "Epoch [1/1], Step[31200/38059], Loss: 2.0493, Perplexity:  7.76\n",
      "Epoch [1/1], Step[31300/38059], Loss: 2.0928, Perplexity:  8.11\n",
      "Epoch [1/1], Step[31400/38059], Loss: 2.1109, Perplexity:  8.26\n",
      "Epoch [1/1], Step[31500/38059], Loss: 2.0594, Perplexity:  7.84\n",
      "Epoch [1/1], Step[31600/38059], Loss: 2.0701, Perplexity:  7.93\n",
      "Epoch [1/1], Step[31700/38059], Loss: 2.1569, Perplexity:  8.64\n",
      "Epoch [1/1], Step[31800/38059], Loss: 2.0629, Perplexity:  7.87\n",
      "Epoch [1/1], Step[31900/38059], Loss: 2.3193, Perplexity: 10.17\n",
      "Epoch [1/1], Step[32000/38059], Loss: 2.2261, Perplexity:  9.26\n",
      "Epoch [1/1], Step[32100/38059], Loss: 2.1176, Perplexity:  8.31\n",
      "Epoch [1/1], Step[32200/38059], Loss: 2.1231, Perplexity:  8.36\n",
      "Epoch [1/1], Step[32300/38059], Loss: 2.0728, Perplexity:  7.95\n",
      "Epoch [1/1], Step[32400/38059], Loss: 2.0236, Perplexity:  7.57\n",
      "Epoch [1/1], Step[32500/38059], Loss: 2.1033, Perplexity:  8.19\n",
      "Epoch [1/1], Step[32600/38059], Loss: 2.1044, Perplexity:  8.20\n",
      "Epoch [1/1], Step[32700/38059], Loss: 2.0605, Perplexity:  7.85\n",
      "Epoch [1/1], Step[32800/38059], Loss: 2.1630, Perplexity:  8.70\n",
      "Epoch [1/1], Step[32900/38059], Loss: 2.0885, Perplexity:  8.07\n",
      "Epoch [1/1], Step[33000/38059], Loss: 1.9892, Perplexity:  7.31\n",
      "Epoch [1/1], Step[33100/38059], Loss: 2.0447, Perplexity:  7.73\n",
      "Epoch [1/1], Step[33200/38059], Loss: 1.9886, Perplexity:  7.30\n",
      "Epoch [1/1], Step[33300/38059], Loss: 2.0560, Perplexity:  7.81\n",
      "Epoch [1/1], Step[33400/38059], Loss: 2.2334, Perplexity:  9.33\n",
      "Epoch [1/1], Step[33500/38059], Loss: 2.1990, Perplexity:  9.02\n",
      "Epoch [1/1], Step[33600/38059], Loss: 2.1521, Perplexity:  8.60\n",
      "Epoch [1/1], Step[33700/38059], Loss: 2.1674, Perplexity:  8.74\n",
      "Epoch [1/1], Step[33800/38059], Loss: 2.1117, Perplexity:  8.26\n",
      "Epoch [1/1], Step[33900/38059], Loss: 2.0938, Perplexity:  8.12\n",
      "Epoch [1/1], Step[34000/38059], Loss: 2.0467, Perplexity:  7.74\n",
      "Epoch [1/1], Step[34100/38059], Loss: 2.1613, Perplexity:  8.68\n",
      "Epoch [1/1], Step[34200/38059], Loss: 2.1398, Perplexity:  8.50\n",
      "Epoch [1/1], Step[34300/38059], Loss: 2.0773, Perplexity:  7.98\n",
      "Epoch [1/1], Step[34400/38059], Loss: 2.1002, Perplexity:  8.17\n",
      "Epoch [1/1], Step[34500/38059], Loss: 2.1626, Perplexity:  8.69\n",
      "Epoch [1/1], Step[34600/38059], Loss: 2.1659, Perplexity:  8.72\n",
      "Epoch [1/1], Step[34700/38059], Loss: 2.1046, Perplexity:  8.20\n",
      "Epoch [1/1], Step[34800/38059], Loss: 2.0613, Perplexity:  7.86\n",
      "Epoch [1/1], Step[34900/38059], Loss: 2.3243, Perplexity: 10.22\n",
      "Epoch [1/1], Step[35000/38059], Loss: 2.1261, Perplexity:  8.38\n",
      "Epoch [1/1], Step[35100/38059], Loss: 2.1317, Perplexity:  8.43\n",
      "Epoch [1/1], Step[35200/38059], Loss: 2.1481, Perplexity:  8.57\n",
      "Epoch [1/1], Step[35300/38059], Loss: 2.0570, Perplexity:  7.82\n",
      "Epoch [1/1], Step[35400/38059], Loss: 2.1547, Perplexity:  8.63\n",
      "Epoch [1/1], Step[35500/38059], Loss: 2.0064, Perplexity:  7.44\n",
      "Epoch [1/1], Step[35600/38059], Loss: 1.9876, Perplexity:  7.30\n",
      "Epoch [1/1], Step[35700/38059], Loss: 2.2037, Perplexity:  9.06\n",
      "Epoch [1/1], Step[35800/38059], Loss: 2.0349, Perplexity:  7.65\n",
      "Epoch [1/1], Step[35900/38059], Loss: 2.0034, Perplexity:  7.41\n",
      "Epoch [1/1], Step[36000/38059], Loss: 2.1322, Perplexity:  8.43\n",
      "Epoch [1/1], Step[36100/38059], Loss: 2.1485, Perplexity:  8.57\n",
      "Epoch [1/1], Step[36200/38059], Loss: 2.2128, Perplexity:  9.14\n",
      "Epoch [1/1], Step[36300/38059], Loss: 2.2835, Perplexity:  9.81\n",
      "Epoch [1/1], Step[36400/38059], Loss: 2.1138, Perplexity:  8.28\n",
      "Epoch [1/1], Step[36500/38059], Loss: 2.1794, Perplexity:  8.84\n",
      "Epoch [1/1], Step[36600/38059], Loss: 2.2678, Perplexity:  9.66\n",
      "Epoch [1/1], Step[36700/38059], Loss: 2.1097, Perplexity:  8.25\n",
      "Epoch [1/1], Step[36800/38059], Loss: 1.9478, Perplexity:  7.01\n",
      "Epoch [1/1], Step[36900/38059], Loss: 2.1394, Perplexity:  8.49\n",
      "Epoch [1/1], Step[37000/38059], Loss: 2.0848, Perplexity:  8.04\n",
      "Epoch [1/1], Step[37100/38059], Loss: 2.0989, Perplexity:  8.16\n",
      "Epoch [1/1], Step[37200/38059], Loss: 2.1464, Perplexity:  8.55\n",
      "Epoch [1/1], Step[37300/38059], Loss: 2.1108, Perplexity:  8.26\n",
      "Epoch [1/1], Step[37400/38059], Loss: 2.1974, Perplexity:  9.00\n",
      "Epoch [1/1], Step[37500/38059], Loss: 2.1585, Perplexity:  8.66\n",
      "Epoch [1/1], Step[37600/38059], Loss: 2.0261, Perplexity:  7.58\n",
      "Epoch [1/1], Step[37700/38059], Loss: 1.9833, Perplexity:  7.27\n",
      "Epoch [1/1], Step[37800/38059], Loss: 2.1502, Perplexity:  8.59\n",
      "Epoch [1/1], Step[37900/38059], Loss: 2.2373, Perplexity:  9.37\n",
      "Epoch [1/1], Step[38000/38059], Loss: 2.0423, Perplexity:  7.71\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.01)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(1, batch_size, hidden_size),\n",
    "              torch.zeros(1, batch_size, hidden_size))\n",
    "    \n",
    "    for i in range(0, data.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = data[:, i:i+seq_length]\n",
    "        targets = data[:, (i+1):(i+1)+seq_length]\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        #clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasn",
   "language": "python",
   "name": "lasn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
